{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3349c49-1d9c-48e2-8c9b-34616f1b57f1",
   "metadata": {},
   "source": [
    "# Boosting-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4a42d-a24b-4f33-b404-f6495d8cfe26",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e5daa-4d4f-442c-9f1b-b813fb8241d5",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression, often referred to as Gradient Boosting Machines (GBM), is a popular ensemble machine learning technique used for regression tasks. It is an extension of the gradient boosting algorithm and is designed to build a predictive model by combining the predictions of multiple weak learners, typically decision trees, to create a strong regression model.\n",
    "\n",
    "In Gradient Boosting Regression, weak learners are trained sequentially, with each learner focusing on the residuals (the differences between the actual target values and the current ensemble's predictions) of the previous learners. The new learner tries to fit the residuals, gradually reducing the errors in the predictions. This process continues until a predefined number of weak learners (trees) is reached or until a convergence criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59bb6b-6610-4ec2-8847-3a8fa01ad608",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2b1d25-ad3d-4f11-ad65-478965fda250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0009078224225023124\n",
      "R-squared: 0.9980425519594686\n"
     ]
    }
   ],
   "source": [
    "import numpy as ny\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate sample data\n",
    "X = ny.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = ny.sin(X).ravel() + ny.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Parameters\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions with the mean of y\n",
    "predictions = ny.full_like(y, ny.mean(y))\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a decision tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X, residuals)\n",
    "    \n",
    "    # Update predictions by adding the tree's prediction (scaled by learning rate)\n",
    "    predictions += learning_rate * tree.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8f494-0017-4564-9e18-8bd7f9fd9b69",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ad7de-39e8-4bda-9944-e41485441358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV as gcv\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid = gcv(GradientBoostingRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c96bd-d0ef-42a6-8504-09061462a457",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a30a3-ab2b-4d00-9768-32e89fdd3bed",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a simple and relatively low-complexity model that performs slightly better than random guessing. Weak learners are often decision trees with limited depth (stumps) or other simple models like linear regression. They are called \"weak\" because they have a limited ability to capture the underlying patterns in the data. Gradient Boosting combines multiple instances of these weak learners to create a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab91b14-346b-460e-8062-c9c49a5fcb1d",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccf2d1-d4bd-40a7-a05c-137bb49f2f51",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to sequentially build an ensemble of weak learners, with each learner correcting the errors of the previous ones. The algorithm focuses on instances where the current ensemble makes mistakes (residuals) and aims to reduce these errors at each step. This process continues until a predefined stopping criterion is met or a specified number of weak learners are added. The final ensemble combines the predictions of all weak learners to produce a strong, accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1015ce-957b-4204-9190-ab75660f1f6c",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0372e-cf1b-4c24-bf2e-e59b3c802d6c",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble of weak learners through an iterative process:\n",
    "* **Initialize:** The first weak learner is trained on the original data or the residuals (differences between actual and predicted values). Initial predictions are often set to the mean of the target values.\n",
    "* **Sequential Training:** In each iteration, a new weak learner is trained on the residuals produced by the current ensemble. The weak learner focuses on reducing the errors made by the current ensemble.\n",
    "* **Weighted Combination:** Each weak learner is assigned a weight based on its performance. Better learners receive higher weights. Predictions from all weak learners are combined using these weights to form the ensemble's final prediction.\n",
    "* **Update Predictions:** The ensemble's predictions are updated by adding the weighted predictions of the newly trained weak learner.\n",
    "* **Repeat:** Steps 2-4 are repeated for a specified number of iterations or until a predefined stopping criterion (e.g., no further improvement) is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed91845-e460-44f2-a15c-6ade73814c84",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4699b4-9a21-4b0d-a940-1bd0e135859d",
   "metadata": {},
   "source": [
    "To construct the mathematical intuition behind Gradient Boosting, you can follow these steps:\n",
    "* **Initialize Predictions:** Start with initial predictions, often set to the mean of the target values for regression problems.\n",
    "* **Compute Residuals:** Calculate the residuals by subtracting the initial predictions from the actual target values.\n",
    "* **Train a Weak Learner:** Fit a weak learner (e.g., decision tree) to the residuals. This weak learner aims to capture the patterns or errors in the data.\n",
    "* **Calculate Weight:** Determine the weight for the newly trained weak learner based on its performance. This weight represents how much the learner's predictions influence the final ensemble.\n",
    "* **Update Predictions:** Update the ensemble's predictions by adding the weighted predictions of the current weak learner.\n",
    "* **Repeat:** Continue the process iteratively. In each iteration, train a new weak learner on the current residuals, calculate its weight, and update the ensemble's predictions.\n",
    "* **Stopping Criterion:** Decide when to stop the iterations. This could be based on the number of iterations, a predefined level of performance, or other criteria.\n",
    "* **Final Prediction:** The final prediction of the Gradient Boosting ensemble is the sum of all weighted weak learner predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
