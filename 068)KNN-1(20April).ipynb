{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d231799-47fe-458c-be28-fe66b6fdc8bc",
   "metadata": {},
   "source": [
    "# KNN-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5a800-7b6a-42e6-afe9-eef092d18465",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ea2f1-8368-4a49-85f9-26f707376970",
   "metadata": {},
   "source": [
    "**KNN stands for \"K-nearest Neighbors.\"** It is a supervised machine learning algorithm used for both classification and regression tasks. In KNN, an object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k-nearest neighbors (hence the name). In regression, it predicts the value of a data point based on the average or weighted average of its k-nearest neighbors' values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf02ea3-93cc-4aba-a439-8d7cf27755d3",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84944ed3-2b62-4207-a4e4-098c380c8427",
   "metadata": {},
   "source": [
    "The choice of the value of K in KNN is crucial and can significantly impact the algorithm's performance. It's typically selected through techniques like cross-validation and grid search. A smaller K may lead to noisy predictions, while a larger K may smooth out the decision boundaries. The choice of K should be based on the specific characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcec21d-15a2-4cde-9aa9-2de717236d2a",
   "metadata": {},
   "source": [
    "#### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4b024-92f8-4a7d-9930-c21740b9a7a3",
   "metadata": {},
   "source": [
    "The main difference is in the type of output they provide:\n",
    "* **KNN Classifier:** It predicts a discrete class label for a given data point based on the majority class among its k-nearest neighbors.\n",
    "* **KNN Regressor:** It predicts a continuous numerical value for a given data point based on the average or weighted average of the values of its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0feb7c-deb8-4e2d-8f87-73cde77829ee",
   "metadata": {},
   "source": [
    "#### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b30885-9899-4e0a-b971-2e1e0ef6d6fb",
   "metadata": {},
   "source": [
    "The performance of KNN can be assessed using various evaluation metrics depending on whether it's used for classification or regression tasks. Common metrics include accuracy, precision, recall, F1-score for classification, and Mean Absolute Error (MAE), Mean Squared Error (MSE), or R-squared for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2ca2e-f3d5-41c6-a41f-1c788974c4b7",
   "metadata": {},
   "source": [
    "#### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165cc2b-b01b-440e-b7a9-c300d6c3f18c",
   "metadata": {},
   "source": [
    "The curse of dimensionality in KNN refers to the phenomenon where the algorithm's performance degrades as the number of features or dimensions in the dataset increases. In high-dimensional spaces, the notion of distance becomes less meaningful, and it becomes challenging to find meaningful nearest neighbors. This can result in increased computational complexity and reduced accuracy. Dimensionality reduction techniques may be used to mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962a7a7-e3d9-40a1-8b68-c7f9c3ca314f",
   "metadata": {},
   "source": [
    "#### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89527542-4d3d-4fa1-85e4-c99a404dcba7",
   "metadata": {},
   "source": [
    "KNN can handle missing values by imputing them. Common approaches include:\n",
    "* Removing data points with missing values.\n",
    "* Replacing missing values with the mean, median, or mode of the feature.\n",
    "* Using imputation methods, such as k-nearest neighbor imputation, to estimate missing values based on the values of their nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899be02-963e-412d-979f-3a003f1766d3",
   "metadata": {},
   "source": [
    "#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cfd88-0bc3-40cc-b411-3576c3cd68ee",
   "metadata": {},
   "source": [
    "KNN Classifier is suitable for problems where the output is a categorical class label, while KNN Regressor is suitable for problems where the output is a continuous numerical value. The choice depends on the nature of the problem we are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28cb00-e6cd-4d6e-8a62-695ddb9e825f",
   "metadata": {},
   "source": [
    "#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b2696-dff6-469c-8078-ae309cc08100",
   "metadata": {},
   "source": [
    "* Strengths:\n",
    "    * Simple to implement.\n",
    "    * Non-parametric (can fit complex decision boundaries).\n",
    "    * Versatile (works for classification and regression).\n",
    "* Weaknesses:\n",
    "    * Computationally expensive for large datasets.\n",
    "    * Sensitive to the choice of K.\n",
    "    * Sensitive to irrelevant features.\n",
    "* Addressing weaknesses may involve using dimensionality reduction, optimizing K, and considering distance weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de969369-6426-4cc9-8b71-4dfd0b372eea",
   "metadata": {},
   "source": [
    "#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e45f3c-e1bd-4ff8-89dc-3b81038fcbca",
   "metadata": {},
   "source": [
    "Euclidean distance calculates the straight-line distance between two points in a Euclidean space, often referred to as the \"as-the-crow-flies\" distance. Manhattan distance, on the other hand, calculates the distance by summing the absolute differences between the coordinates of two points. Euclidean distance is sensitive to diagonal movements, while Manhattan distance is not and is more suitable for grid-like structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644b5c6-a159-4ba2-a305-ffb934d04ba6",
   "metadata": {},
   "source": [
    "#### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0abe0-658e-4b81-9419-62367a7fee02",
   "metadata": {},
   "source": [
    "Feature scaling is essential in KNN because it ensures that all features contribute equally to the distance calculations. Features with larger scales or ranges can dominate the distance calculation, leading to biased results. Common techniques for feature scaling include Min-Max scaling (scaling features to a specified range) and z-score normalization (scaling features to have mean 0 and standard deviation 1). Feature scaling helps improve the algorithm's performance and stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
