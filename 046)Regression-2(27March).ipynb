{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c89595-8a17-43a7-9b14-89df35ae0370",
   "metadata": {},
   "source": [
    "# Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aed7ea-03f5-4e93-a85a-ff79eac8e31a",
   "metadata": {},
   "source": [
    "#### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162fa5c-36ab-42bd-a4de-cb38a97248a5",
   "metadata": {},
   "source": [
    "**R-squared (coefficient of determination)** is a statistical measure used to assess how well the independent variables (predictors) explain the variability of the dependent variable in a linear regression model. It represents the proportion of the variance in the dependent variable that is explained by the independent variables.\n",
    "* It's calculated as: R-squared = 1 - (SSR/SST), *where SSR is the sum of squared residuals (the difference between predicted and actual values), and SST is the total sum of squares (the difference between actual values and the mean of the dependent variable).*\n",
    "* R-squared ranges from 0 to 1. A value closer to 1 indicates that a larger portion of the variability in the dependent variable is explained by the model's predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6921d9f-1679-43f1-b03a-6478587e8bf9",
   "metadata": {},
   "source": [
    "#### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cc6cf-90f5-4e3c-ab37-b9a4bdce46c7",
   "metadata": {},
   "source": [
    "**Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model. It penalizes the addition of irrelevant predictors that might artificially increase R-squared. It's especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "* Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)],\n",
    "*where n is the number of observations and p is the number of predictors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da080d38-c57d-4726-b5e0-9937b8e979ab",
   "metadata": {},
   "source": [
    "#### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee4eb8-67a7-4a12-9b87-6c3d4ae6d610",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate when we're comparing models with different numbers of predictors. It helps us to determine if the additional predictors actually improve the model's fit and explain variability, considering the potential risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30a770-5a03-4bd3-a30a-5dd5e8901be1",
   "metadata": {},
   "source": [
    "#### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb58ef0-ffdb-4811-b8db-f67cca985552",
   "metadata": {},
   "source": [
    "* **RMSE (Root Mean Squared Error):** The square root of the average of squared differences between predicted and actual values. It penalizes larger errors more heavily.\n",
    "* **MSE (Mean Squared Error):** The average of squared differences between predicted and actual values.\n",
    "* **MAE (Mean Absolute Error):** The average of absolute differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce63701-460d-4587-b931-7c53d800471d",
   "metadata": {},
   "source": [
    "#### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127e9d1-d62c-4602-90e3-302a582ffad7",
   "metadata": {},
   "source": [
    "* **Advantages:**\n",
    "    * All three metrics are common and easy to understand.\n",
    "    * RMSE and MSE give more weight to larger errors, which might be important in some applications.\n",
    "    * MAE is more robust to outliers.\n",
    "\n",
    "* **Disadvantages:**\n",
    "    * All metrics don't directly indicate whether errors are overestimates or underestimates.\n",
    "    * RMSE and MSE can be sensitive to outliers.\n",
    "    * The choice of metric depends on the specific problem and the emphasis on different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa08b1-64a3-41a9-bcd1-2393d34a97c5",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b7096-44e5-4d8e-9abd-f454d7c8fa90",
   "metadata": {},
   "source": [
    "*Lasso (Least Absolute Shrinkage and Selection Operator) regularization* adds a penalty term to the linear regression cost function that's proportional to the absolute values of the regression coefficients. It can drive some coefficients to exactly zero, effectively performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd60453-0775-4062-aade-93914d16288d",
   "metadata": {},
   "source": [
    "#### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6159d5c-e3e6-4bd9-8195-b023d91d86e5",
   "metadata": {},
   "source": [
    "Regularization techniques like Lasso and Ridge add a penalty to the magnitude of the coefficients, preventing them from becoming too large. This helps in reducing overfitting by simplifying the model and avoiding fitting noise in the data.\n",
    "\n",
    "    Example: In a linear regression predicting house prices, regularization can shrink the coefficients of less relevant features, preventing them from exerting excessive influence on the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccfdb8-0274-48a2-8a4a-ef3ee5fc4604",
   "metadata": {},
   "source": [
    "#### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac711e16-8ff1-4a4b-9a0b-89e9ed291600",
   "metadata": {},
   "source": [
    "* Choice of hyperparameters (like the regularization strength) can be challenging and might require cross-validation.\n",
    "* Regularization might oversimplify the model by pushing some coefficients too close to zero, potentially ignoring important variables.\n",
    "* In cases where all predictors are relevant, regularization might not be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c6bfe-4836-4fc2-8ebe-f0d7f6e2038d",
   "metadata": {},
   "source": [
    "#### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3164e-3ff7-4acb-8436-7858d45ce1a8",
   "metadata": {},
   "source": [
    "Both RMSE and MAE measure prediction accuracy, but RMSE penalizes larger errors more. In our case, Model B with an MAE of 8 is likely the better performer, as it indicates smaller average errors. However, the choice depends on the problem's specifics and the importance of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fb58c-2d1d-48dd-bc15-57a4738ea039",
   "metadata": {},
   "source": [
    "#### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ff1a9-3b0e-42ab-be63-a67315755732",
   "metadata": {},
   "source": [
    "Choosing between Ridge and Lasso depends on the situation. Model A (Ridge) with a regularization parameter of 0.1 might be preferred if we suspect all predictors are relevant but need some degree of regularization. Model B (Lasso) with a regularization parameter of 0.5 is more appropriate if we suspect that many predictors are irrelevant and can be effectively removed.\n",
    "\n",
    "Trade-offs: Ridge tends to shrink coefficients towards zero without making them exactly zero, while Lasso can lead to some coefficients being exactly zero, effectively performing feature selection. This trade-off affects model interpretability and complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
