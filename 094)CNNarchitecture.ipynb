{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5edd53-7924-4684-a0cc-df4bb98ce6c7",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c4bad-337f-4574-80cd-d1b8022bad39",
   "metadata": {},
   "source": [
    "### TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62f1d9-5c8a-4218-950a-efa2a6aea2cc",
   "metadata": {},
   "source": [
    "#### Q1. Describe the purpose and benefits of pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8016464-1e4d-4bf9-a7c6-bcf16b77142d",
   "metadata": {},
   "source": [
    "Pooling, in Convolutional Neural Networks (CNNs), serves several important purposes and offers several benefits:\n",
    "* **Purpose:**\n",
    "    * **Dimensionality Reduction:** Pooling reduces the spatial dimensions (width and height) of feature maps, effectively downscaling the information. This helps in managing computational complexity and memory requirements, which is crucial in deep networks.\n",
    "    * **Translation Invariance:** Pooling helps make the network more robust to small translations or distortions in the input. By summarizing local information into a single value, the network becomes less sensitive to precise spatial locations of features.\n",
    "    * **Feature Hierarchy:** Pooling, when used in conjunction with convolutional layers, creates a hierarchy of features. Higher-level features are learned from the output of convolutional layers, while pooling layers capture more abstract and higher-level information.\n",
    "    * **Computation Efficiency:** Pooling reduces the amount of computation required during both forward and backward passes, making training and inference faster and more efficient.\n",
    "* **Benefits:**\n",
    "    * **Reduction of Overfitting:** By downsampling the feature maps, pooling reduces the risk of overfitting, as it reduces the number of parameters in the network.\n",
    "    * **Improved Computation:** Smaller feature maps after pooling require less computation, which speeds up training and inference.\n",
    "    * **Enhanced Generalization:** Pooling helps the network generalize better by focusing on essential features while ignoring less relevant details.\n",
    "    * **Shift Invariance:** Especially in max pooling, the network becomes partially invariant to small translations, which can be advantageous in recognizing objects regardless of their exact position within the receptive field.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388770e-c95b-43e3-8782-6fbbe1f046fd",
   "metadata": {},
   "source": [
    "#### Q2. Explain the difference between Min pooling and Max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8811b56-3a4d-4f56-8e7f-f526b6a0fb40",
   "metadata": {},
   "source": [
    "Max pooling and Min pooling are two common types of pooling operations in CNNs, and they differ in how they select the representative value from a local region:\n",
    "\n",
    "* **Max Pooling:**\n",
    "    - In max pooling, for each local region (typically a 2x2 or 3x3 window), the maximum value is selected as the representative value for that region.\n",
    "    - Max pooling is often used to retain the most prominent feature within each local region.\n",
    "    - It emphasizes the most active feature in the region, which can be useful for detecting specific patterns.\n",
    "\n",
    "* **Min Pooling:**\n",
    "    - In min pooling, the minimum value from the local region is chosen as the representative value.\n",
    "    - Min pooling is used when we want to emphasize the presence of smaller features or the least prominent information in the local region.\n",
    "    - It can be used to detect the least active feature in the region, which might be useful in certain scenarios.\n",
    "\n",
    "The choice between max pooling and min pooling depends on the specific problem and the characteristics of the data being processed. Max pooling is more common and often produces better results for many tasks, as it tends to capture the most relevant features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee08e7-1f61-4771-8338-9808ee6d9591",
   "metadata": {},
   "source": [
    "#### Q3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab149e03-692e-4853-a094-4619d039fe5e",
   "metadata": {},
   "source": [
    "Padding is a technique used in CNNs to control the spatial dimensions of the output feature maps after convolution or pooling operations. It involves adding extra rows and columns of zeros (or other values) around the input data before applying the operation. Padding is significant for several reasons:\n",
    "* **Preserving Spatial Information:**\n",
    "    - Padding allows the output feature map to have the same spatial dimensions as the input. This is crucial when maintaining spatial information is important.\n",
    "    - It ensures that information at the edges of the input is considered during convolution or pooling.\n",
    "* **Controlling Output Size:**\n",
    "    - Padding enables us to control the size of the output feature map. We can choose the amount of padding to achieve the desired output size, which is essential for network design and computational resource management.\n",
    "* **Edge Information:**\n",
    "    - Padding ensures that the information at the edges of the input is considered during convolution or pooling. Without padding, the outer pixels might be underrepresented in the output feature map.\n",
    "* There are two common types of padding:\n",
    "    - **Zero-padding (Same Padding):** Here, zeros are added around the input data. It helps maintain the spatial dimensions and is often used when preserving spatial information is crucial.\n",
    "    - **Valid Padding (No Padding):** In this case, no padding is added, and the spatial dimensions of the output feature map are reduced. It's used when dimensionality reduction is desired or when the spatial information at the edges is less important.\n",
    "\n",
    "The choice between zero-padding and valid-padding depends on the network architecture, the nature of the problem, and whether spatial information preservation is a priority.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05943e-11ca-4baa-bdee-47438f67e788",
   "metadata": {},
   "source": [
    "#### Q4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099454f-a807-4d30-b010-dc87aa36ff6e",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding have distinct effects on the output feature map size in CNNs:\n",
    "* **Zero-Padding (Same Padding):**\n",
    "    - Increases the spatial dimensions of the output feature map.\n",
    "    - Helps maintain the spatial information from the input.\n",
    "    - Commonly used when we want the output feature map to have the same spatial dimensions as the input.\n",
    "    - Output size (O) can be calculated as:\n",
    "          **O = [(W - F + 2P) / S] + 1,** *where:*\n",
    "      - *W is the input size.*\n",
    "      - *F is the filter size.*\n",
    "      - *P is the amount of zero-padding.*\n",
    "      - *S is the stride.*\n",
    "* **Valid-Padding (No Padding):**\n",
    "    - Does not add any extra rows or columns to the input.\n",
    "    - Reduces the spatial dimensions of the output feature map.\n",
    "    - Commonly used when dimensionality reduction is desired or when the spatial information at the edges is less important.\n",
    "    - Output size (O) can be calculated as:\n",
    "          **O = [(W - F) / S] + 1,** *where:*\n",
    "      - *W is the input size.*\n",
    "      - *F is the filter size.*\n",
    "      - *S is the stride.*\n",
    "\n",
    "In summary, zero-padding increases the output size and preserves spatial information, while valid-padding reduces the output size and may discard information from the edges of the input. The choice between these padding strategies depends on the specific requirements of the CNN architecture and the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ae5a5-9891-464d-9b48-33a5a9b9036b",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939253c8-0f3c-46e6-8ca7-81b0fe959f67",
   "metadata": {},
   "source": [
    "#### Q1. Provide a concise overview of LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb11c4d-a031-4d0c-ba51-1014a5d6f7c3",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in the late 1990s. It was designed for handwritten digit recognition and has played a significant role in the advancement of deep learning. Here's a concise overview of the LeNet-5 architecture:\n",
    "- **Input Layer:** LeNet-5 takes grayscale images as input, typically of size 32x32 pixels.\n",
    "- **Convolutional Layers:** LeNet-5 consists of two pairs of convolutional layers followed by activation functions. These convolutional layers learn hierarchical features from the input image.\n",
    "- **Max Pooling Layers:** After each pair of convolutional layers, max-pooling layers are applied to downsample the spatial dimensions of the feature maps and enhance translation invariance.\n",
    "- **Fully Connected Layers:** Following the convolutional and pooling layers, LeNet-5 has three fully connected layers. These layers flatten the output from the previous layers and map it to the output class probabilities.\n",
    "- **Activation Functions:** Sigmoid activation functions were primarily used in LeNet-5.\n",
    "- **Output Layer:** The final output layer typically has 10 units (one for each digit class) and uses the softmax activation function to produce class probabilities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680887c3-5539-40ee-bb9f-ec1e6445a83e",
   "metadata": {},
   "source": [
    "#### Q2. Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cbcd4-b660-4aec-873b-e86aef970c1c",
   "metadata": {},
   "source": [
    "1. **Convolutional Layers:** The convolutional layers are responsible for learning local patterns and features in the input image. The learned features become more abstract as we move deeper into the network.\n",
    "2. **Max Pooling Layers:** Max pooling layers downsample the spatial dimensions of the feature maps while preserving essential information. They help make the network invariant to small translations or distortions in the input.\n",
    "3. **Fully Connected Layers:** These layers serve as a classifier, taking the high-level features learned from the convolutional layers and making class predictions. The fully connected layers are responsible for capturing global patterns and relationships in the data.\n",
    "4. **Activation Functions:** In the original LeNet-5, sigmoid activation functions were used. While modern CNNs often use rectified linear units (ReLUs), sigmoid activations were common at the time of LeNet-5's development.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4304b-d368-44a0-b684-a3d71cd7a1a1",
   "metadata": {},
   "source": [
    "#### Q3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2492d1d-9514-4f95-bd85-f68047420871",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- **Pioneering Work:** LeNet-5 was one of the earliest successful CNN architectures, setting the stage for modern deep learning research.\n",
    "- **Effective for Handwriting Recognition:** It was highly effective for its original purpose of recognizing handwritten digits, achieving state-of-the-art results at the time.\n",
    "\n",
    "**Limitations:**\n",
    "- **Limited Depth:** LeNet-5 has a relatively shallow architecture compared to modern CNNs. Deeper networks can learn more complex and abstract features, which is crucial for tasks with large and diverse datasets.\n",
    "- **Activation Functions:** The use of sigmoid activations in LeNet-5 can result in the vanishing gradient problem, limiting its ability to train very deep networks effectively.\n",
    "- **Small Input Size:** LeNet-5 was designed for small 32x32 input images. It may not perform well on tasks that require high-resolution inputs, such as object detection or recognition in natural images.\n",
    "- **Simplicity:** While simplicity can be an advantage, LeNet-5's architecture may not capture the complexity of modern image recognition tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6668e-b26c-4cf5-a029-9940ae440701",
   "metadata": {},
   "source": [
    "#### Q4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e41db-e0c0-4e30-85a4-40f3ee2b6a62",
   "metadata": {},
   "source": [
    "##### LeNet-5 using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cc72ad-e4d1-4b82-bf9b-6375bd0fd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9297293821123362\n",
      "Epoch 2, Loss: 0.34765644957309466\n",
      "Epoch 3, Loss: 0.19032202784750443\n",
      "Epoch 4, Loss: 0.1386182621192894\n",
      "Epoch 5, Loss: 0.11206938845437092\n",
      "Epoch 6, Loss: 0.09449441493776783\n",
      "Epoch 7, Loss: 0.0827250789626559\n",
      "Epoch 8, Loss: 0.07468811215272844\n",
      "Epoch 9, Loss: 0.06742035615434652\n",
      "Epoch 10, Loss: 0.06202100622288978\n",
      "Finishing Training\n",
      "Accuracy on test dataset: 98.19%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Architechture\n",
    "class lenet5 (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lenet5, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1,6,5)      # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.p1 = nn.MaxPool2d(2,2)     # Max pooling with 2x2 window\n",
    "        self.c2 = nn.Conv2d(6,16,5)     # 6 input channel, 16 output channels, 5x5 kernel\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.p2 = nn.MaxPool2d(2,2)     # Max pooling with 2x2 window\n",
    "        self.f1 = nn.Linear(16*4*4,120) # 16 feature maps, 4x4 spatial size\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.f2 = nn.Linear(120,84)\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.f3 = nn.Linear(84,10)      # 10 output classes for MNIST\n",
    "    def forward(self,x):\n",
    "        x = self.p1(self.a1(self.c1(x)))\n",
    "        x = self.p2(self.a2(self.c2(x)))\n",
    "        x = x.view(-1,16*4*4)           # Flatten the output\n",
    "        x = self.a3(self.f1(x))\n",
    "        x = self.a4(self.f2(x))\n",
    "        x = self.f3(x)\n",
    "        return x\n",
    "    \n",
    "# Loading MNIST Dataset\n",
    "transform = transform.Compose([transform.ToTensor(), transform.Normalize((0.5,),(0.5,))])\n",
    "Train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "Test  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train = DataLoader(Train, batch_size=64, shuffle=True)\n",
    "test  = DataLoader(Test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model Prepartions\n",
    "model = lenet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "for e in range(10):\n",
    "    loss = 0\n",
    "    for i, d in enumerate(train,0):\n",
    "        inputs , labels = d\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        l = criterion(outputs,labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss+= l.item()\n",
    "    print(f\"Epoch {e + 1}, Loss: {loss / len(train)}\")\n",
    "print('Finishing Training')\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for d in test:\n",
    "        image, labels = d\n",
    "        output = model(image)\n",
    "        _,predict = torch.max(output.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "acc = 100*correct/total\n",
    "print(f\"Accuracy on test dataset: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad211274-be43-40fd-acc3-e1971d104817",
   "metadata": {},
   "source": [
    "##### LeNet-5 using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbe3e1b-d092-4ede-9f62-bad6ac064737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 11s - loss: 0.2006 - accuracy: 0.9386 - 11s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 10s - loss: 0.0654 - accuracy: 0.9792 - 10s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 9s - loss: 0.0454 - accuracy: 0.9856 - 9s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 9s - loss: 0.0368 - accuracy: 0.9881 - 9s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 9s - loss: 0.0284 - accuracy: 0.9909 - 9s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 9s - loss: 0.0261 - accuracy: 0.9916 - 9s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 9s - loss: 0.0219 - accuracy: 0.9927 - 9s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 9s - loss: 0.0183 - accuracy: 0.9939 - 9s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 9s - loss: 0.0161 - accuracy: 0.9948 - 9s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 9s - loss: 0.0157 - accuracy: 0.9951 - 9s/epoch - 5ms/step\n",
      "Test accuracy: 98.79%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Loading and Scaling Datasets\n",
    "(x_train, y_train) , (x_test, y_test) = datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "x_train, x_test = x_train.reshape(-1,28,28,1).astype('float32'), x_test.reshape(-1,28,28,1).astype('float32')\n",
    "\n",
    "# Model Creations\n",
    "model = models.Sequential([layers.Conv2D(6,(5,5), activation='relu', input_shape=(28,28,1)), layers.MaxPool2D((2,2)),\n",
    "                           layers.Conv2D(16,(5,5), activation='relu'), layers.MaxPool2D((2,2)), layers.Flatten(),\n",
    "                           layers.Dense(120, activation='relu'), layers.Dense(84, activation='relu'), layers.Dense(10)])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(x_train,y_train, epochs=10, verbose=2)\n",
    "\n",
    "# Model Evaluation\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dec94-e3e7-4854-84af-3f93dfb289c7",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "## TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f10847-2c17-48b6-af07-e643e1726b1f",
   "metadata": {},
   "source": [
    "#### Q1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40be9ac-bf2c-4ca8-934d-d209917933cd",
   "metadata": {},
   "source": [
    "AlexNet is a deep convolutional neural network (CNN) architecture that gained significant attention and marked a breakthrough in image classification when it won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Here's an overview of the AlexNet architecture:\n",
    "1. **Input Layer:** AlexNet takes color images as input, typically of size 224x224 pixels.\n",
    "2. **Convolutional Layers:** It consists of five convolutional layers, each followed by a Rectified Linear Unit (ReLU) activation function. These layers are responsible for learning hierarchical features from the input images. The number of filters increases with depth.\n",
    "3. **Max Pooling Layers:** After the first two convolutional layers, max-pooling layers are applied to downsample the spatial dimensions of the feature maps.\n",
    "4. **Normalization Layers:** Local Response Normalization (LRN) layers are used to normalize the output of some convolutional layers. This helps enhance the network's ability to generalize.\n",
    "5. **Dropout Layers:** Dropout layers are introduced after the fully connected layers to prevent overfitting during training.\n",
    "6. **Fully Connected Layers:** AlexNet has three fully connected layers. The first two have 4,096 units each, and the last one has 1,000 units (corresponding to the 1,000 ImageNet classes). The final fully connected layer produces class predictions using a softmax activation function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b61ac-4ab9-44c9-a5bc-34c5e714f4a0",
   "metadata": {},
   "source": [
    "#### Q2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a49fb-ef6e-43be-9465-6af197d30351",
   "metadata": {},
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance:\n",
    "1. **Deep Architecture:** AlexNet was one of the first deep CNNs to have multiple convolutional layers stacked on top of each other. This depth allowed the network to learn hierarchical features from low-level edges and textures to high-level object parts and semantics.\n",
    "2. **ReLU Activation:** AlexNet used Rectified Linear Units (ReLU) as activation functions instead of traditional sigmoid or tanh activations. ReLU activations accelerated training by mitigating the vanishing gradient problem.\n",
    "3. **Local Response Normalization (LRN):** LRN layers were applied after some convolutional layers to provide local contrast normalization, which improved the network's generalization ability.\n",
    "4. **Overlapping Max Pooling:** AlexNet used max-pooling layers with a stride smaller than the pooling window size, leading to overlapping pooling regions. This increased the network's ability to capture spatial hierarchies.\n",
    "5. **Dropout Regularization:** Dropout layers were introduced after the fully connected layers to prevent overfitting. During training, dropout randomly deactivates neurons, which reduces co-dependency among them.\n",
    "6. **Data Augmentation:** Data augmentation techniques, such as cropping and horizontal flipping of training images, were used to artificially increase the size of the training dataset. This helped improve the model's robustness to variations in input data.\n",
    "7. **Large-Scale Training Data:** AlexNet was trained on a large dataset, ImageNet, which contained over a million images spanning 1,000 different classes. The availability of this extensive training dataset was crucial for the network's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca9fa9-d2e4-48d6-8875-0784f038b6fb",
   "metadata": {},
   "source": [
    "#### Q3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1318a48-bdb9-4b99-9bfd-3778c571144f",
   "metadata": {},
   "source": [
    "- **Convolutional Layers:** The convolutional layers in AlexNet play a central role in feature extraction. They learn various low-level and high-level features from the input images, gradually building a hierarchical representation of visual information. Convolutional layers are responsible for capturing patterns, textures, and object parts.\n",
    "- **Pooling Layers:** Max pooling layers in AlexNet downsample the spatial dimensions of the feature maps, reducing computational complexity and enhancing translation invariance. Overlapping pooling regions help capture spatial hierarchies within the feature maps.\n",
    "- **Fully Connected Layers:** The fully connected layers at the end of AlexNet serve as a classifier. They take the high-level features learned by the convolutional layers and make class predictions. The final fully connected layer produces class probabilities using a softmax activation function. These layers capture global patterns and relationships in the data and provide the network's output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59098324-7e16-43ce-b322-509dba021449",
   "metadata": {},
   "source": [
    "#### Q4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0a0a3-32f9-4be3-bb3e-9298ed993c53",
   "metadata": {},
   "source": [
    "##### LeNet-5 using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495970dd-93a6-4b52-b516-e4148849f37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, Loss: 2.296324372596448\n",
      "Epoch 2, Loss: 2.0316340918736078\n",
      "Epoch 3, Loss: 1.6874255785704269\n",
      "Epoch 4, Loss: 1.49628219229486\n",
      "Epoch 5, Loss: 1.3600778068270525\n",
      "Epoch 6, Loss: 1.24536047856826\n",
      "Epoch 7, Loss: 1.1388245041641738\n",
      "Epoch 8, Loss: 1.0397641702991007\n",
      "Epoch 9, Loss: 0.9535676575530215\n",
      "Epoch 10, Loss: 0.8738080864138615\n",
      "Finishing Training\n",
      "Accuracy on test dataset: 68.7%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Architechture\n",
    "class alexnet(nn.Module):\n",
    "    def __init__(self, num=10):\n",
    "        super(alexnet, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(3,64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                      nn.Conv2d(64,192, kernel_size=5, padding=2), nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2), \n",
    "                                      nn.Conv2d(192,384, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(384,256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(256,256, kernel_size=3, padding=1), nn.ReLU(inplace=True), \n",
    "                                      nn.MaxPool2d(kernel_size=3,stride=2),)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.classifier = nn.Sequential(nn.Dropout(), nn.Linear(256*6*6,4096), nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(), nn.Linear(4096,4096), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(4096,num),)\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Loading CIFAR-10 Dataset\n",
    "transform = transform.Compose([transform.Resize((224,224)),transform.ToTensor(),\n",
    "                               transform.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])])\n",
    "Train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "Test  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train = DataLoader(Train, batch_size=64, shuffle=True)\n",
    "test  = DataLoader(Test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model Prepartions\n",
    "model = alexnet(num=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "for e in range(10):\n",
    "    loss = 0\n",
    "    for i, d in enumerate(train,0):\n",
    "        inputs , labels = d\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        l = criterion(outputs,labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss+= l.item()\n",
    "    print(f\"Epoch {e + 1}, Loss: {loss / len(train)}\")\n",
    "print('Finishing Training')\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for d in test:\n",
    "        image, labels = d\n",
    "        output = model(image)\n",
    "        _,predict = torch.max(output.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "acc = 100*correct/total\n",
    "print(f\"Accuracy on test dataset: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fd447-6400-4ac5-8c0f-bd323509fc39",
   "metadata": {},
   "source": [
    "##### LeNet-5 using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03e13f4-3823-4965-af85-4725e1f4ff5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 - 371s - loss: 2.3027 - accuracy: 0.0996 - 371s/epoch - 237ms/step\n",
      "Epoch 2/10\n",
      "1563/1563 - 366s - loss: 2.3028 - accuracy: 0.0984 - 366s/epoch - 234ms/step\n",
      "Epoch 3/10\n",
      "1563/1563 - 364s - loss: 2.3028 - accuracy: 0.0978 - 364s/epoch - 233ms/step\n",
      "Epoch 4/10\n",
      "1563/1563 - 365s - loss: 2.3028 - accuracy: 0.0984 - 365s/epoch - 234ms/step\n",
      "Epoch 5/10\n",
      "1563/1563 - 366s - loss: 2.3028 - accuracy: 0.0996 - 366s/epoch - 234ms/step\n",
      "Epoch 6/10\n",
      "1563/1563 - 376s - loss: 2.3028 - accuracy: 0.0973 - 376s/epoch - 241ms/step\n",
      "Epoch 7/10\n",
      "1563/1563 - 381s - loss: 2.3028 - accuracy: 0.0989 - 381s/epoch - 244ms/step\n",
      "Epoch 8/10\n",
      "1563/1563 - 371s - loss: 2.3028 - accuracy: 0.0982 - 371s/epoch - 237ms/step\n",
      "Epoch 9/10\n",
      "1563/1563 - 380s - loss: 2.3028 - accuracy: 0.0990 - 380s/epoch - 243ms/step\n",
      "Epoch 10/10\n",
      "1563/1563 - 395s - loss: 2.3028 - accuracy: 0.0959 - 395s/epoch - 253ms/step\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Loading and Scaling Datasets\n",
    "(x_train, y_train) , (x_test, y_test) = datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# Model Creations\n",
    "model = models.Sequential([layers.Conv2D(96,(3,3), strides=(4,4), padding='same',activation='relu',input_shape=(32,32,3)),\n",
    "                           layers.MaxPool2D((2,2), strides=(2,2)),\n",
    "                           layers.Conv2D(256,(3,3), padding='same', activation='relu'),\n",
    "                           layers.MaxPool2D((2,2), strides=(2,2)),\n",
    "                           layers.Conv2D(384,(3,3), padding='same', activation='relu'),\n",
    "                           layers.Conv2D(384,(3,3), padding='same', activation='relu'),\n",
    "                           layers.Conv2D(256,(3,3), padding='same', activation='relu'),\n",
    "                           layers.MaxPool2D((2,2), strides=(2,2)), \n",
    "                           layers.Flatten(),\n",
    "                           layers.Dense(4096, activation='relu'),\n",
    "                           layers.Dense(4096, activation='relu'),\n",
    "                           layers.Dense(10)])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(x_train,y_train, epochs=10, verbose=2)\n",
    "\n",
    "# Model Evaluation\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
