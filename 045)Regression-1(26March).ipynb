{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f971d93e-64b1-42ae-bb7f-1d091483ac58",
   "metadata": {},
   "source": [
    "# Regression-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ab4b0-782f-4fd7-a349-48c53838095c",
   "metadata": {},
   "source": [
    "#### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95702b-b04f-455d-b815-3f96f3033622",
   "metadata": {},
   "source": [
    "Simple Linear Regression involves predicting a dependent variable (response) based on a single independent variable (predictor). It assumes a linear relationship between the variables, which can be represented by a straight line equation (y = mx + b).\n",
    "* Example: Predicting a person's salary (dependent variable) based on their years of experience (independent variable).\n",
    "\n",
    "Multiple Linear Regression, on the other hand, extends the concept to multiple independent variables. It predicts the dependent variable based on a combination of two or more independent variables, assuming a linear relationship between them.\n",
    "* Example: Predicting a house's price (dependent variable) based on its size, number of bedrooms, and location (independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ff9fb-d90d-4493-852f-ef3f7987702f",
   "metadata": {},
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2ad08-d750-4049-8906-f39c4ceb68cb",
   "metadata": {},
   "source": [
    "* Linearity: The relationship between independent and dependent variables is linear.\n",
    "* Independence: Residuals (the differences between predicted and actual values) are independent of each other.\n",
    "* Homoscedasticity: Residuals have constant variance across all levels of predictor variables.\n",
    "* Normality: Residuals are normally distributed.\n",
    "\n",
    "We can check these assumptions using various diagnostic techniques such as residual plots, normality plots, and statistical tests. If assumptions are violated, we might need to transform variables or use more advanced regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748abf22-b3fd-434a-8a06-8d125de9ab25",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62868874-39cd-4714-9390-756e5d624799",
   "metadata": {},
   "source": [
    "In the equation y = mx + b:\n",
    "* Slope (m): Represents the change in the dependent variable for a unit change in the independent variable. It quantifies the relationship's strength.\n",
    "* Intercept (b): Represents the value of the dependent variable when the independent variable is zero. It might or might not hold practical meaning depending on the context.\n",
    "\n",
    "Example: In a simple linear regression predicting salary based on years of experience, the slope indicates the average increase in salary for each additional year of experience, and the intercept represents the starting salary for someone with zero years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2ae9f-95ed-4bd6-ad2c-730463070032",
   "metadata": {},
   "source": [
    "#### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9c124-61ad-4307-aa03-81f78acee18c",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models. It iteratively adjusts the model's parameters in the opposite direction of the gradient (slope) of the loss function, aiming to reach the minimum loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e9043-f504-41c1-9b74-145a7551bdbe",
   "metadata": {},
   "source": [
    "#### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90eda9-07de-44c9-a52c-236cef67b50f",
   "metadata": {},
   "source": [
    "* Multiple Linear Regression involves multiple independent variables, capturing complex relationships and accounting for multiple factors that might influence the dependent variable. It has the form: y = b0 + b1x1 + b2x2 + ... + bn*xn.\n",
    "* Simple Linear Regression has only one independent variable, making it more straightforward but less representative of real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044db72-a7cd-46f6-8d03-5e2176b7cd53",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34b2d6-9599-413e-acad-2300bf21d1ea",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more independent variables in a multiple regression model are highly correlated, making it difficult to determine their individual effects on the dependent variable.\n",
    "* Detection: Calculate the correlation matrix between independent variables. High correlation coefficients indicate multicollinearity.\n",
    "* Addressing: Remove one of the correlated variables, use dimensionality reduction techniques (like Principal Component Analysis), or combine correlated variables into a single variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24873dc3-ab2d-4e28-b671-7ffa849c0921",
   "metadata": {},
   "source": [
    "#### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8ce6a-708f-4388-ab66-c26701651028",
   "metadata": {},
   "source": [
    "Polynomial Regression extends linear regression by fitting a polynomial equation to the data, allowing for curved relationships between variables. The equation becomes y = b0 + b1x + b2x^2 + ... + bn*x^n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185c90b-839e-44b7-8f44-f35f82a99660",
   "metadata": {},
   "source": [
    "#### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492380b5-7716-45e6-a2bd-d483d2d1da7b",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "* Can model more complex relationships that aren't linear.\n",
    "* Can better fit data with bends and curves.\n",
    "* Doesn't rely on linear assumptions.\n",
    "\n",
    "Disadvantages:\n",
    "* Prone to overfitting, especially with higher-degree polynomials.\n",
    "* Extrapolation can be unreliable.\n",
    "* Interpretability becomes challenging with higher-degree polynomials.\n",
    "\n",
    "Use Polynomial Regression when there's clear evidence of non-linear relationships in the data, but be cautious of overfitting and the potential lack of generalizability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
