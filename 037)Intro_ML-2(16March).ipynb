{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61b3329-312b-4166-8257-0d65a296a8c4",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c534cc-5f41-4a17-923c-0ea02cdb5d89",
   "metadata": {},
   "source": [
    "#### Q1 Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7014e1-104c-4161-a59d-471cb9517877",
   "metadata": {},
   "source": [
    "* Overfitting: Overfitting occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor generalization to new, unseen data. Consequences include high training accuracy but poor test accuracy. Mitigation techniques include reducing model complexity, increasing training data, and using regularization.\n",
    "* Underfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. The model performs poorly on both training and test data. Consequences include low training and test accuracy. Mitigation involves using more complex models, increasing feature engineering, and improving data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4079c-3605-4d3f-9293-68ae43e230d4",
   "metadata": {},
   "source": [
    "#### Q2 How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80111632-a7d9-4ab9-813f-fa4038516c12",
   "metadata": {},
   "source": [
    "To reduce overfitting:\n",
    "1. Simplify Model Complexity: Use simpler models with fewer parameters.\n",
    "2. Regularization: Add regularization terms to the loss function to penalize complex models.\n",
    "3. Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data.\n",
    "4. Increase Training Data: More diverse and representative data can help the model generalize better.\n",
    "5. Feature Selection/Engineering: Focus on relevant features and reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b9428-d268-45db-85ec-46f87626b9fc",
   "metadata": {},
   "source": [
    "#### Q3 Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f34e5-644a-4ee2-ae59-f0df168bb1de",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple to capture the complexity of the underlying data patterns. Scenarios include using linear models for nonlinear data, models with too few parameters, or models without sufficient feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c492c-ebb3-4bb0-a57d-9d349bd9e4fe",
   "metadata": {},
   "source": [
    "#### Q4 Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb334fe-024e-401a-91ee-e1a7ae3bd03c",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff refers to the balance between a model's bias (error due to oversimplification) and variance (error due to sensitivity to fluctuations). High-bias models underfit, while high-variance models overfit. The goal is to find the optimal level of complexity that minimizes both bias and variance to achieve good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52758d4-5e5c-4367-8ad0-2989e429b4a7",
   "metadata": {},
   "source": [
    "#### Q5 Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31379fd5-3196-4d12-bb03-dda002cf1596",
   "metadata": {},
   "source": [
    "* Overfitting Detection:\n",
    "    * Monitoring Training and Validation Loss: If training loss decreases but validation loss increases, overfitting may be occurring.\n",
    "    * Using Learning Curves: Plotting the training and validation loss over epochs can help identify overfitting.\n",
    "* Underfitting Detection:\n",
    "    * Poor Training and Validation Performance: If both training and validation loss are high, the model might be underfitting.\n",
    "    * Comparing to Baseline Models: If the model's performance is significantly worse than simple baseline models, underfitting could be the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded51f51-b3c5-43c4-a26a-642833fbbd98",
   "metadata": {},
   "source": [
    "#### Q6 Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93dad-6e2a-4661-90fa-04b80a1c0062",
   "metadata": {},
   "source": [
    "* Bias: Bias is error due to oversimplification. High bias models (underfitting) have poor training and validation performance due to inadequate complexity. Examples include a linear model for nonlinear data.\n",
    "* Variance: Variance is error due to sensitivity to fluctuations. High variance models (overfitting) perform well on training but poorly on validation due to capturing noise. Examples include high-degree polynomial models for limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c702486-c13a-41d8-a932-dc02aefa88a9",
   "metadata": {},
   "source": [
    "#### Q7 What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c8ace-ccf6-4e75-909d-0d85fe9b912e",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting by adding penalty terms to the loss function. Common regularization techniques include:\n",
    "* L1 Regularization (Lasso): Adds the sum of absolute values of coefficients as a penalty term. It encourages sparsity in feature selection, as some coefficients may become exactly zero.\n",
    "* L2 Regularization (Ridge): Adds the sum of squared coefficients as a penalty term. It tends to shrink coefficients towards zero without necessarily making them zero.\n",
    "* Elastic Net Regularization: A combination of L1 and L2 regularization. It provides a balance between feature selection and coefficient shrinkage.\n",
    "* Dropout: Used in neural networks, dropout randomly drops a fraction of units/neurons during training, making the network less reliant on specific neurons and thus less prone to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
