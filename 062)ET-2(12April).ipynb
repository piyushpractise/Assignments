{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edbab2e-8a25-4b7e-9fa2-f7eeacdea79b",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9cc67-6660-440a-a8d9-17082ec4f89e",
   "metadata": {},
   "source": [
    "#### Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bb951-a3d4-4e8c-9cae-b5cdb579a252",
   "metadata": {},
   "source": [
    "Bagging reduces overfitting in decision trees by introducing diversity among the individual trees in the ensemble. Each decision tree is trained on a bootstrapped (randomly sampled with replacement) subset of the training data, which means that each tree sees a slightly different perspective of the data. When predictions are combined, the ensemble tends to generalize better to unseen data because the diversity among the trees helps reduce the impact of overfitting in individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192d0a5-f6a2-48a6-9934-df18a56bda88",
   "metadata": {},
   "source": [
    "#### Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd117fd-b66b-4869-9730-7c7923770957",
   "metadata": {},
   "source": [
    "* **Advantages:**\n",
    "    * Diverse base learners can capture different aspects of the data, leading to improved ensemble performance.\n",
    "    * Using various types of base learners can increase the ensemble's robustness.\n",
    "* **Disadvantages:**\n",
    "    * Diversity among base learners can sometimes lead to increased computational complexity.\n",
    "    * Combining predictions from very different base learners may require more sophisticated aggregation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661a276-7be7-4585-85d6-655123d60f5a",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc3e11-f6a9-49d9-b109-f35a5a22d4fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "The choice of the base learner can impact the bias-variance tradeoff in bagging. In general:\n",
    "   * If the base learner has high variance (e.g., a deep decision tree), bagging tends to reduce variance, leading to lower overall variance and a potential increase in bias.\n",
    "   * If the base learner has high bias (e.g., a shallow decision tree), bagging can reduce bias, leading to a decrease in overall bias and a potential increase in variance.\n",
    "\n",
    "The impact on the bias-variance tradeoff depends on the specific characteristics of the base learner and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623f3c5-5884-4cd0-a740-f37d39c7f3a2",
   "metadata": {},
   "source": [
    "#### Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97e0f1-be48-4f21-8e0a-250ea5d22708",
   "metadata": {},
   "source": [
    "Yes, bagging can be used for both classification and regression tasks.\n",
    "* In classification, each base learner typically produces a class prediction, and the ensemble combines these predictions using majority voting.\n",
    "* In regression, each base learner produces a numeric prediction, and the ensemble combines these predictions using averaging or another aggregation method.\n",
    "* The primary difference lies in how the predictions are combined, but the general bagging framework remains similar for both tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e8a2b-87b3-4576-95af-30ee9e2d1af5",
   "metadata": {},
   "source": [
    "#### Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc40bdf-7354-426b-8706-fd3830e05a0a",
   "metadata": {},
   "source": [
    "The ensemble size (i.e., the number of base learners or decision trees) in bagging can impact the ensemble's performance. Generally:\n",
    "* Increasing the ensemble size tends to improve performance up to a certain point, as it increases the diversity among the base learners.\n",
    "* After a certain threshold, adding more models may have diminishing returns, and computational resources become a limiting factor.\n",
    "\n",
    "The optimal ensemble size can vary depending on the problem and dataset. It's often determined through experimentation and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28484f-c9b3-424b-bd16-6bfaf7870c8a",
   "metadata": {},
   "source": [
    "#### Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7167d671-9fda-49ce-8158-a535ae44cbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Base Model: 0.9350\n",
      "Accuracy of Bagging Model: 0.9500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading dataset & Spliting them\n",
    "ds = load_breast_cancer()\n",
    "x = ds.data\n",
    "y = ds.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.35,random_state=45)\n",
    "\n",
    "# Creating a model\n",
    "bmodel = DecisionTreeClassifier(random_state=45) # Base Model\n",
    "n_estimator = 10 \n",
    "bagging_model = BaggingClassifier(bmodel, n_estimators = n_estimator, random_state=45)\n",
    "\n",
    "# Training, Prediction and Evalaution\n",
    "bmodel.fit(x_train,y_train)\n",
    "bagging_model.fit(x_train,y_train)\n",
    "by_pred = bmodel.predict(x_test)\n",
    "y_pred = bagging_model.predict(x_test)\n",
    "acc = accuracy_score(y_test,by_pred)\n",
    "bagg_acc = accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy of Base Model: {acc:.4f}\")\n",
    "print(f\"Accuracy of Bagging Model: {bagg_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
