{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e155f9f4-de13-4b1b-a36c-a09049d8804b",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f0c1b-18b9-412d-aaf3-90a63b3260cb",
   "metadata": {},
   "source": [
    "#### Objective:\n",
    "##### Assess understanding of optimization algorithms in artificial neural networks. Evaluate the application and comparison of different optimizers. Enhance knowledge of optimizers' impact on model convergence and performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72740577-4c18-4cf8-840d-38595a9762ef",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "#### Part 1: Understanding Optimizer\n",
    "1. What is the role of optimization algorithms in artificial neural networks? Why are they necessary?\n",
    "2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements.\n",
    "3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges?\n",
    "4. Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do they impact convergence and model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5dcb0-06af-4e6a-a6ef-81516391652e",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "**Answer 1.** Optimization algorithms play a crucial role in artificial neural networks by iteratively adjusting the model's parameters (weights and biases) to minimize a predefined loss function. Their primary purpose is to find the optimal set of parameters that result in the lowest possible loss, effectively training the neural network. Optimization is necessary because neural networks are typically high-dimensional and non-convex optimization problems, making it infeasible to find the optimal solution analytically. Optimization algorithms automate the process of finding these parameters efficiently through iterative updates.\n",
    "\n",
    "---\n",
    "**Answer 2.** Gradient descent is a fundamental optimization algorithm used in training neural networks. It involves computing the gradient (derivative) of the loss function with respect to the model parameters and updating the parameters in the opposite direction of the gradient to minimize the loss. Variants of gradient descent include:\n",
    "- **Stochastic Gradient Descent (SGD)**: In SGD, a random subset (mini-batch) of the training data is used to compute the gradient and update the parameters in each iteration. It introduces randomness and can converge faster than batch gradient descent. However, it can exhibit noisy updates.\n",
    "- **Mini-Batch Gradient Descent**: This is a compromise between batch gradient descent and SGD. It uses a small, fixed-size mini-batch from the training data to compute the gradient and update the parameters. It combines some benefits of both batch and stochastic approaches.\n",
    "- **Batch Gradient Descent**: In this method, the entire training dataset is used to compute the gradient and update the parameters in each iteration. It provides a more stable but potentially slower convergence compared to SGD.\n",
    "\n",
    "Differences and tradeoffs:\n",
    "- Convergence Speed: SGD and mini-batch GD often converge faster than batch GD because they make more frequent updates. Batch GD, on the other hand, may take longer but can have more stable updates.\n",
    "- Memory Requirements: Batch GD requires more memory as it processes the entire dataset in each iteration. SGD and mini-batch GD have lower memory requirements.\n",
    "\n",
    "---\n",
    "**Answer 3.** Traditional gradient descent methods face several challenges:\n",
    "- **Slow Convergence**: Traditional gradient descent can converge slowly, especially when dealing with deep neural networks and high-dimensional data. It may take a long time to reach a minimum.\n",
    "- **Local Minima**: The optimization landscape of neural networks is non-convex, leading to the problem of getting stuck in local minima.\n",
    "\n",
    "Modern optimizers address these challenges:\n",
    "- **Faster Convergence**: Optimizers like Adam, RMSprop, and AdaGrad incorporate adaptive learning rates and momentum, which help them converge faster by adjusting the step sizes based on the gradient's magnitude.\n",
    "- **Escape Local Minima**: Techniques like momentum and adaptive learning rates allow optimizers to escape local minima by adding momentum to the parameter updates or reducing the learning rate in problematic regions.\n",
    "\n",
    "---\n",
    "**Answer 4.** Momentum and learning rate are key concepts in optimization algorithms:\n",
    "- **Momentum**: Momentum is a technique that helps accelerate convergence by adding a fraction of the previous update to the current update. It smoothens the optimization trajectory and helps the optimizer overcome local minima and saddle points. A higher momentum value increases the impact of previous updates.\n",
    "- **Learning Rate**: The learning rate is a hyperparameter that controls the step size during parameter updates. It determines how quickly or slowly a neural network learns. A higher learning rate can result in faster convergence, but it may lead to overshooting the optimal solution. A lower learning rate can help the model converge more steadily but might take longer.\n",
    "\n",
    "The choice of learning rate and momentum values can significantly impact the training process. Too high of a learning rate can lead to divergence, while too low of a learning rate can result in slow convergence. Proper tuning is necessary to balance convergence speed and stability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9a71d-9781-420a-a955-92f7adf84a41",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "#### Part 2: Optimizer Techniques\n",
    "1. Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations and scenarios where it is most suitable.\n",
    "2. Describe the concept of the Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks.\n",
    "    3. Explain the concept of the RMSprop optimizer and how it addresses the challenges of adaptive learning rates. Compare it with Adam and discuss their relative strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e996cbe-3a6e-407b-bfbf-34cde0e46f11",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "**Answer 1** Stochastic Gradient Descent is an optimization algorithm used to train machine learning models, including neural networks. It is an extension of traditional gradient descent. In SGD, instead of computing the gradient of the loss function with respect to the entire training dataset (as in batch gradient descent), the gradient is computed for a small random subset, or mini-batch, of the training data in each iteration.\n",
    "* **Advantages**:\n",
    "    - **Faster Convergence**: SGD often converges faster than batch gradient descent because it makes more frequent updates to the model parameters. These frequent updates can help escape local minima and saddle points more quickly.\n",
    "    - **Lower Memory Requirements**: Since it processes only a mini-batch of data at a time, SGD requires less memory compared to batch gradient descent, making it suitable for large datasets.\n",
    "    - **Regularization Effect**: The inherent randomness in selecting mini-batches introduces a form of implicit regularization, which can help prevent overfitting to some extent.\n",
    "* **Limitations**:\n",
    "    - **Noisy Updates**: SGD's updates can be noisy due to the randomness of the mini-batch selection. This noise can lead to oscillations in the optimization trajectory.\n",
    "    - **Slower Convergence in Certain Cases**: SGD's convergence can be slower if the mini-batches are too small or if the learning rate is not properly tuned.\n",
    "* **Suitability**:\n",
    "    - SGD is most suitable for large datasets where batch gradient descent may be computationally expensive.\n",
    "    - It is also suitable for online learning scenarios, where data is continuously streaming, and the model needs to adapt incrementally.\n",
    "    - Proper tuning of the learning rate and mini-batch size is crucial for its effectiveness.\n",
    "\n",
    "---\n",
    "**Answer 2** Adam (short for Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of momentum and adaptive learning rates. It maintains two moving averages, one for the gradient's first moment (mean) and another for the second moment (uncentered variance). These moving averages are used to adaptively adjust the learning rates for each parameter.\n",
    "- **Benefits**:\n",
    "     - **Fast Convergence**: Adam often converges quickly due to its adaptive learning rates. It can automatically adjust the learning rates for each parameter based on the magnitude of the gradients.\n",
    "     - **Escape Local Minima**: Like momentum, Adam helps escape local minima by adding a momentum term to parameter updates.\n",
    "     - **Regularization**: The adaptive learning rates act as a form of regularization, providing stability during training. \n",
    "- **Drawbacks**:\n",
    "     - **Complexity**: Adam has more hyperparameters to tune compared to SGD, which can make it more challenging to set the right values.\n",
    "     - **Sensitivity to Hyperparameters**: It can be sensitive to the choice of hyperparameters, and poorly tuned hyperparameters may result in suboptimal performance.\n",
    "\n",
    "---\n",
    "**Answer 3** Root Mean Square Propagation (RMSprop) is an optimization algorithm that addresses the challenge of adaptive learning rates. It computes moving averages of the squared gradients for each parameter and uses these averages to adjust the learning rates. RMSprop aims to provide more stable convergence compared to vanilla SGD.\n",
    "- **Benefits**:\n",
    "     - **Stability**: RMSprop provides stability during training by adapting the learning rates based on the history of gradients. It reduces the risk of diverging or overshooting.\n",
    "     - **No Need for Manual Learning Rate Tuning**: Unlike SGD, RMSprop does not require manual tuning of the learning rate. It automatically adapts to the problem.\n",
    "- **Drawbacks**:\n",
    "     - **Hyperparameter Sensitivity**: While RMSprop is less sensitive to learning rate tuning than SGD, it can still be sensitive to other hyperparameters.\n",
    "     - **May Converge to Suboptimal Solutions**: In some cases, RMSprop may converge to suboptimal solutions compared to algorithms like Adam.\n",
    "- **Comparison with Adam**:\n",
    "     - Both RMSprop and Adam use moving averages for adaptive learning rates, but Adam also incorporates momentum.\n",
    "     - Adam can sometimes converge faster due to its momentum term, but it has more hyperparameters to tune.\n",
    "     - RMSprop is a good choice when you want an adaptive learning rate method with fewer hyperparameters to manage.\n",
    "     \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36000f3f-9197-4332-ab36-5ac4b7112feb",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black\">\n",
    "\n",
    "#### Part 3: Applying Optimizers\n",
    "1. Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. Train the model on a suitable dataset and compare their impact on model convergence and performance.\n",
    "2. Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. Consider factors such as convergence speed, stability, and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d334e3-2608-4883-9a4f-b6e895b7e12e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 1.1484 - accuracy: 0.3896 - val_loss: 0.8896 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8568 - accuracy: 0.6104 - val_loss: 0.7247 - val_accuracy: 0.9500\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6185 - accuracy: 0.8052 - val_loss: 0.5219 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5187 - accuracy: 0.8442 - val_loss: 0.4862 - val_accuracy: 0.9500\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4344 - accuracy: 0.8052 - val_loss: 0.3702 - val_accuracy: 0.9500\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4096 - accuracy: 0.7792 - val_loss: 0.5716 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4123 - accuracy: 0.7792 - val_loss: 0.3189 - val_accuracy: 0.7500\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3660 - accuracy: 0.7662 - val_loss: 0.4601 - val_accuracy: 0.5500\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3063 - accuracy: 0.8571 - val_loss: 0.2182 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2997 - accuracy: 0.8571 - val_loss: 0.1916 - val_accuracy: 0.9500\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2478 - accuracy: 0.9221 - val_loss: 0.3052 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2246 - accuracy: 0.9351 - val_loss: 0.1493 - val_accuracy: 0.9500\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2084 - accuracy: 0.9481 - val_loss: 0.1775 - val_accuracy: 0.9500\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1891 - accuracy: 0.9351 - val_loss: 0.2148 - val_accuracy: 0.9500\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2345 - accuracy: 0.8571 - val_loss: 0.1969 - val_accuracy: 0.9000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2797 - accuracy: 0.8571 - val_loss: 0.1782 - val_accuracy: 0.9500\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2425 - accuracy: 0.8961 - val_loss: 0.1783 - val_accuracy: 0.9500\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2671 - accuracy: 0.8831 - val_loss: 0.1216 - val_accuracy: 0.9500\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3086 - accuracy: 0.8831 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3394 - accuracy: 0.8701 - val_loss: 0.4421 - val_accuracy: 0.7500\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2598 - accuracy: 0.8831 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9481 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1250 - accuracy: 0.9740 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1184 - accuracy: 0.9740 - val_loss: 0.1642 - val_accuracy: 0.9500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1258 - accuracy: 0.9481 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1205 - accuracy: 0.9740 - val_loss: 0.1371 - val_accuracy: 0.9500\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1084 - accuracy: 0.9481 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1309 - accuracy: 0.9610 - val_loss: 0.0549 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9740 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1642 - accuracy: 0.9351 - val_loss: 0.0766 - val_accuracy: 0.9500\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3511 - accuracy: 0.8312 - val_loss: 0.1449 - val_accuracy: 0.9500\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1671 - accuracy: 0.9351 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2228 - accuracy: 0.9221 - val_loss: 0.1658 - val_accuracy: 0.9000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2783 - accuracy: 0.8961 - val_loss: 0.3579 - val_accuracy: 0.8500\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1664 - accuracy: 0.9221 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2305 - accuracy: 0.8701 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2099 - accuracy: 0.8961 - val_loss: 0.1025 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2051 - accuracy: 0.9091 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3445 - accuracy: 0.8442 - val_loss: 0.2755 - val_accuracy: 0.8500\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2658 - accuracy: 0.8571 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1579 - accuracy: 0.9351 - val_loss: 0.1374 - val_accuracy: 0.9500\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1314 - accuracy: 0.9610 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1138 - accuracy: 0.9740 - val_loss: 0.1108 - val_accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1197 - accuracy: 0.9351 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1169 - accuracy: 0.9481 - val_loss: 0.2826 - val_accuracy: 0.8500\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1469 - accuracy: 0.9610 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2616 - accuracy: 0.9091 - val_loss: 0.3090 - val_accuracy: 0.8500\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1941 - accuracy: 0.9221 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2077 - accuracy: 0.9091 - val_loss: 0.2715 - val_accuracy: 0.8500\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2206 - accuracy: 0.9091 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "While using Stochastic Gradient Descent  Test Loss: 0.0732  and Test Accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 35ms/step - loss: 0.1018 - accuracy: 0.9610 - val_loss: 0.0820 - val_accuracy: 0.9500\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0969 - accuracy: 0.9481 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0914 - accuracy: 0.9610 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0893 - accuracy: 0.9740 - val_loss: 0.0752 - val_accuracy: 0.9500\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0894 - accuracy: 0.9610 - val_loss: 0.0615 - val_accuracy: 0.9500\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0829 - accuracy: 0.9870 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0912 - accuracy: 0.9610 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0830 - accuracy: 0.9610 - val_loss: 0.0643 - val_accuracy: 0.9500\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9610 - val_loss: 0.0757 - val_accuracy: 0.9500\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0903 - accuracy: 0.9610 - val_loss: 0.0625 - val_accuracy: 0.9500\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0800 - accuracy: 0.9870 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9740 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0856 - accuracy: 0.9740 - val_loss: 0.0547 - val_accuracy: 0.9500\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0797 - accuracy: 0.9870 - val_loss: 0.0534 - val_accuracy: 0.9500\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0817 - accuracy: 0.9740 - val_loss: 0.0620 - val_accuracy: 0.9500\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9740 - val_loss: 0.0517 - val_accuracy: 0.9500\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9870 - val_loss: 0.0512 - val_accuracy: 0.9500\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0778 - accuracy: 0.9870 - val_loss: 0.0511 - val_accuracy: 0.9500\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0777 - accuracy: 0.9870 - val_loss: 0.0590 - val_accuracy: 0.9500\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0773 - accuracy: 0.9870 - val_loss: 0.0618 - val_accuracy: 0.9500\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9870 - val_loss: 0.0508 - val_accuracy: 0.9500\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9870 - val_loss: 0.0522 - val_accuracy: 0.9500\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0754 - accuracy: 0.9870 - val_loss: 0.0519 - val_accuracy: 0.9500\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0822 - accuracy: 0.9610 - val_loss: 0.0671 - val_accuracy: 0.9500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0830 - accuracy: 0.9481 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.0528 - val_accuracy: 0.9500\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0801 - val_accuracy: 0.9500\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9740 - val_loss: 0.0602 - val_accuracy: 0.9500\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0735 - accuracy: 0.9870 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0757 - accuracy: 0.9870 - val_loss: 0.0487 - val_accuracy: 0.9500\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0738 - accuracy: 0.9870 - val_loss: 0.0505 - val_accuracy: 0.9500\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0786 - accuracy: 0.9740 - val_loss: 0.0647 - val_accuracy: 0.9500\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0761 - accuracy: 0.9870 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0745 - accuracy: 0.9870 - val_loss: 0.0549 - val_accuracy: 0.9500\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0802 - accuracy: 0.9610 - val_loss: 0.0766 - val_accuracy: 0.9500\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.0582 - val_accuracy: 0.9500\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0791 - accuracy: 0.9610 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0731 - accuracy: 0.9870 - val_loss: 0.0586 - val_accuracy: 0.9500\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0730 - accuracy: 0.9610 - val_loss: 0.0778 - val_accuracy: 0.9500\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0737 - accuracy: 0.9610 - val_loss: 0.0698 - val_accuracy: 0.9500\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9870 - val_loss: 0.0527 - val_accuracy: 0.9500\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0746 - accuracy: 0.9870 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.0571 - val_accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0704 - accuracy: 0.9870 - val_loss: 0.0666 - val_accuracy: 0.9500\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9610 - val_loss: 0.0753 - val_accuracy: 0.9500\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0716 - accuracy: 0.9610 - val_loss: 0.0629 - val_accuracy: 0.9500\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0776 - accuracy: 0.9870 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0754 - accuracy: 0.9740 - val_loss: 0.0565 - val_accuracy: 0.9500\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0718 - accuracy: 0.9870 - val_loss: 0.0634 - val_accuracy: 0.9500\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0691 - accuracy: 0.9870 - val_loss: 0.0596 - val_accuracy: 0.9500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "While using Adam optimizer  Test Loss: 0.0693  and Test Accuracy: 0.9623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 35ms/step - loss: 0.0916 - accuracy: 0.9610 - val_loss: 0.0640 - val_accuracy: 0.9500\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0818 - accuracy: 0.9740 - val_loss: 0.0700 - val_accuracy: 0.9500\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9610 - val_loss: 0.0990 - val_accuracy: 0.9500\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0804 - accuracy: 0.9870 - val_loss: 0.0699 - val_accuracy: 0.9500\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0929 - accuracy: 0.9610 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0774 - accuracy: 0.9610 - val_loss: 0.0611 - val_accuracy: 0.9500\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9610 - val_loss: 0.0731 - val_accuracy: 0.9500\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0702 - accuracy: 0.9870 - val_loss: 0.0735 - val_accuracy: 0.9500\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0819 - accuracy: 0.9740 - val_loss: 0.0979 - val_accuracy: 0.9500\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9740 - val_loss: 0.0977 - val_accuracy: 0.9500\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0877 - accuracy: 0.9481 - val_loss: 0.0779 - val_accuracy: 0.9500\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0754 - accuracy: 0.9870 - val_loss: 0.0671 - val_accuracy: 0.9500\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9870 - val_loss: 0.0810 - val_accuracy: 0.9500\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0882 - accuracy: 0.9481 - val_loss: 0.0864 - val_accuracy: 0.9500\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0732 - accuracy: 0.9740 - val_loss: 0.0694 - val_accuracy: 0.9500\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0715 - accuracy: 0.9740 - val_loss: 0.0514 - val_accuracy: 0.9500\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0824 - accuracy: 0.9610 - val_loss: 0.1307 - val_accuracy: 0.9500\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0824 - accuracy: 0.9481 - val_loss: 0.0784 - val_accuracy: 0.9500\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0713 - accuracy: 0.9610 - val_loss: 0.1050 - val_accuracy: 0.9500\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0743 - accuracy: 0.9740 - val_loss: 0.0634 - val_accuracy: 0.9500\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0781 - accuracy: 0.9610 - val_loss: 0.0573 - val_accuracy: 0.9500\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0736 - accuracy: 0.9870 - val_loss: 0.0458 - val_accuracy: 0.9500\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0729 - accuracy: 0.9740 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0986 - accuracy: 0.9740 - val_loss: 0.0832 - val_accuracy: 0.9500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0862 - accuracy: 0.9481 - val_loss: 0.1081 - val_accuracy: 0.9500\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0894 - accuracy: 0.9610 - val_loss: 0.0798 - val_accuracy: 0.9500\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0777 - accuracy: 0.9481 - val_loss: 0.0701 - val_accuracy: 0.9500\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0812 - accuracy: 0.9870 - val_loss: 0.0664 - val_accuracy: 0.9500\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9740 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9740 - val_loss: 0.1118 - val_accuracy: 0.9500\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0875 - accuracy: 0.9610 - val_loss: 0.0595 - val_accuracy: 0.9500\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0687 - accuracy: 0.9870 - val_loss: 0.0664 - val_accuracy: 0.9500\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0815 - accuracy: 0.9740 - val_loss: 0.0500 - val_accuracy: 0.9500\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0777 - accuracy: 0.9740 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0764 - accuracy: 0.9740 - val_loss: 0.0544 - val_accuracy: 0.9500\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9740 - val_loss: 0.0549 - val_accuracy: 0.9500\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0710 - accuracy: 0.9870 - val_loss: 0.0631 - val_accuracy: 0.9500\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0837 - accuracy: 0.9740 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0702 - accuracy: 0.9870 - val_loss: 0.0867 - val_accuracy: 0.9500\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0715 - accuracy: 0.9740 - val_loss: 0.0887 - val_accuracy: 0.9500\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0673 - accuracy: 0.9740 - val_loss: 0.0488 - val_accuracy: 0.9500\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0720 - accuracy: 0.9740 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.0953 - val_accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9610 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0746 - accuracy: 0.9870 - val_loss: 0.0877 - val_accuracy: 0.9500\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9740 - val_loss: 0.0913 - val_accuracy: 0.9500\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0819 - accuracy: 0.9740 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0687 - accuracy: 0.9740 - val_loss: 0.0766 - val_accuracy: 0.9500\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9740 - val_loss: 0.0502 - val_accuracy: 0.9500\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0757 - accuracy: 0.9740 - val_loss: 0.0466 - val_accuracy: 0.9500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9811\n",
      "----------------------------------------------------------------------------------------------------\n",
      "While using RMSprop optimizer  Test Loss: 0.0523  and Test Accuracy: 0.9811\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Answer 1\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# Load the Iris dataset\n",
    "ds = load_iris()\n",
    "x, y = ds.data, ds.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=42)\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=4))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the model with different optimizers and Evaluate the model on the test data\n",
    "\n",
    " # 1. Stochastic Gradient Descent (SGD)\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('-' * 100)\n",
    "print(\"While using Stochastic Gradient Descent \",f\"Test Loss: {test_loss:.4f}\",f\" and Test Accuracy: {test_accuracy:.4f}\")\n",
    "print('-' * 100)\n",
    "\n",
    "# 2. Adam optimizer\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('-' * 100)\n",
    "print(\"While using Adam optimizer \",f\"Test Loss: {test_loss:.4f}\",f\" and Test Accuracy: {test_accuracy:.4f}\")\n",
    "print('-' * 100)\n",
    "\n",
    "# 3. RMSprop optimizer\n",
    "rmsprop_optimizer = RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=rmsprop_optimizer, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('-' * 100)\n",
    "print(\"While using RMSprop optimizer \",f\"Test Loss: {test_loss:.4f}\",f\" and Test Accuracy: {test_accuracy:.4f}\")\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea38df-b819-457c-9628-1c55117e984e",
   "metadata": {},
   "source": [
    "When choosing the appropriate optimizer for a neural network architecture and task, several considerations and tradeoffs come into play. The choice of optimizer can significantly impact the training process and the performance of the resulting model. Here are some key factors to consider:\n",
    "* Convergence Speed:\n",
    "    * Different optimizers have different convergence speeds. Some optimizers, like Adam and RMSprop, often converge faster than traditional optimizers like Stochastic Gradient Descent (SGD). This can be crucial when training large and deep neural networks.\n",
    "    * However, faster convergence doesn't always mean better results. Rapid convergence can sometimes lead to overshooting and convergence to poor local minima. Slower optimizers like SGD may provide more stable convergence.\n",
    "* Stability:\n",
    "    * The choice of optimizer can affect the stability of the training process. Optimizers like Adam and RMSprop often handle a wide range of learning rates automatically, making them more stable choices.\n",
    "    * On the other hand, SGD may require more careful learning rate tuning to ensure stability, especially in deep networks.\n",
    "* Generalization Performance:\n",
    "    * Generalization performance is crucial. Some optimizers, like Adam, can lead to models that generalize well to unseen data. However, there's a risk of overfitting, especially when using larger batch sizes.\n",
    "    * Smaller batch sizes, along with optimizers like SGD, can sometimes lead to better generalization because they introduce more randomness into the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
