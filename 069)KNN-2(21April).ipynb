{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccae503-8a4e-4f71-b52f-3a302ecbc9a4",
   "metadata": {},
   "source": [
    "# KNN-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29ad7d-db31-43aa-a3b1-af6e13f3ad7e",
   "metadata": {},
   "source": [
    "#### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3b2e5-0343-4928-b5dd-8cdac28e82cd",
   "metadata": {},
   "source": [
    "* The main difference between Euclidean and Manhattan distances lies in how they measure distance in feature space:\n",
    "    * Euclidean distance calculates the straight-line or \"as-the-crow-flies\" distance between two points. It considers both horizontal and vertical movements.\n",
    "    * Manhattan distance, also known as L1 distance, calculates distance by summing the absolute differences between the coordinates of two points. It only considers horizontal and vertical movements (no diagonal).\n",
    "* The choice of distance metric can affect KNN's performance:\n",
    "    * Euclidean distance is sensitive to diagonal movements and is suitable when features have continuous and natural interpretations.\n",
    "    * Manhattan distance is less sensitive to diagonal movements and is suitable when features represent steps or grid-like structures.\n",
    "    * The choice should align with the data's characteristics; using the wrong metric can lead to suboptimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21890ca9-01dc-4f91-84e8-80bf1e14c715",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf9e47-9274-4953-90cb-eba1b1916b60",
   "metadata": {},
   "source": [
    "The optimal value of k in KNN depends on the dataset and problem. Common techniques to determine the optimal k value include:\n",
    "* Cross-validation: Split the dataset into training and validation sets, and test different values of k. Choose the k that yields the best performance on the validation set.\n",
    "* Grid search: Evaluate the model's performance for a range of k values using cross-validation and choose the best k.\n",
    "* Elbow method: Plot the model's performance (e.g., accuracy or error) against different k values and look for an \"elbow point\" where performance stabilizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7cec2-0f6d-422d-bb4c-7b3e14c6958d",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc7796-e1c1-41b6-95cf-4949506f000c",
   "metadata": {},
   "source": [
    "The choice of distance metric can significantly impact KNN's performance:\n",
    "* Euclidean distance may work well when features are continuous and naturally lend themselves to Euclidean geometry.\n",
    "* Manhattan distance is more suitable when features represent steps or grid-like movements.\n",
    "* The choice depends on the data's characteristics. Experimentation and domain knowledge can guide the selection of the appropriate metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b91b69-9f3e-46f9-8e6c-f945c9cd26b1",
   "metadata": {},
   "source": [
    "#### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c141ad-b930-40ec-8953-d4dd9a5b6189",
   "metadata": {},
   "source": [
    "* Common hyperparameters in KNN include:\n",
    "    * K (number of neighbors)\n",
    "    * Distance metric\n",
    "    * Weighting scheme (uniform or distance-weighted)\n",
    "* The choice of k affects model complexity; smaller k values lead to more flexible models prone to noise, while larger k values provide smoother decision boundaries.\n",
    "* The distance metric influences how features are weighted in distance calculations.\n",
    "* Hyperparameters can be tuned using techniques like grid search or random search to find the combination that yields the best performance through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ff1b8-b30c-408d-8035-89ca07da2945",
   "metadata": {},
   "source": [
    "#### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbdbf7-8fc5-4e36-83d8-5008093ffd74",
   "metadata": {},
   "source": [
    "* The size of the training set can impact KNN's performance. A smaller training set may lead to overfitting, while a larger training set can help generalize better.\n",
    "* Techniques to optimize the training set size include:\n",
    "    * Cross-validation to assess model performance with different training set sizes.\n",
    "    * Resampling techniques (e.g., bootstrapping) to create larger training sets.\n",
    "    * Data augmentation to artificially increase the training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272ac66-0d9f-4038-b3ac-473fdb125b93",
   "metadata": {},
   "source": [
    "#### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d117d48-2e43-4344-af5c-109c05814936",
   "metadata": {},
   "source": [
    "* Drawbacks of KNN include:\n",
    "    * Computational inefficiency for large datasets.\n",
    "    * Sensitivity to the choice of k.\n",
    "    * Sensitivity to irrelevant features.\n",
    "* To overcome these drawbacks:\n",
    "    * Consider dimensionality reduction techniques for high-dimensional data.\n",
    "    * Optimize k through cross-validation.\n",
    "    * Perform feature selection to remove irrelevant features.\n",
    "    * Use advanced data structures like KD-Trees or Ball Trees for efficiency.\n",
    "    * Implement parallelization for faster computations on multi-core processors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
