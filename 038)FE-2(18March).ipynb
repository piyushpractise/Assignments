{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa070d26-1c0f-4071-aef4-4f9f00c70a40",
   "metadata": {},
   "source": [
    "# Feature Engineering-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c727d9-2eec-4164-9733-f8984aa5da61",
   "metadata": {},
   "source": [
    "#### Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac78bf3-8a40-4c85-9d03-0fb2da19d3dc",
   "metadata": {},
   "source": [
    "The Filter method in feature selection involves evaluating the intrinsic properties of individual features without involving a specific machine learning model. It works by ranking or scoring features based on certain statistical measures, such as correlation, mutual information, or variance. Features are selected or retained based on their scores, without considering their interactions with the target variable or other features.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset of students with features 'age', 'study_hours', and 'grade'. We can use the correlation coefficient between each feature and the 'grade' as a filter. Features with higher correlation coefficients are deemed more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526d7590-ba11-42f4-9427-08c099573592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features are ['study_hours', 'grade']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame({'age': [18, 20, 22, 19, 21], 'study_hours': [5, 6, 4, 7, 5], 'grade': [85, 90, 75, 95, 80]})\n",
    "cor = data.corr()['grade']\n",
    "selected = cor[cor>0.7].index.tolist()\n",
    "print('Selected Features are', selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af9d02-4127-4fb4-b1e9-8be0d76c07fb",
   "metadata": {},
   "source": [
    "#### Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c0090-312a-409e-961b-0fd09cfd13b3",
   "metadata": {},
   "source": [
    "The Wrapper method involves using a machine learning model to evaluate the performance of different subsets of features. It works by iteratively training and evaluating the model on different feature combinations, which allows it to capture feature interactions and their impact on model performance. Unlike the Filter method, the Wrapper method considers how features affect the model's predictive performance.\n",
    "\n",
    "Example:\n",
    "Consider the same student dataset. We can use a wrapper method like Recursive Feature Elimination (RFE) with a linear regression model to iteratively select the best subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa70a92a-10d1-4a4a-a832-325b66bd807b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Featues are: Index(['study_hours'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "data = pd.DataFrame({'age': [18, 20, 22, 19, 21], 'study_hours': [5, 6, 4, 7, 5], 'grade': [85, 90, 75, 95, 80]})\n",
    "x= data.drop('grade', axis=1)\n",
    "y= data['grade']\n",
    "ref = RFE(lr(),n_features_to_select = 1)\n",
    "ref.fit(x,y)\n",
    "selected = x.columns[ref.support_]\n",
    "print('Selected Featues are:',selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfb26f-28e9-4f3d-a2bd-5333a94d11fa",
   "metadata": {},
   "source": [
    "#### Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0e8b6-4e6b-4419-987f-a5fb5478fd0c",
   "metadata": {},
   "source": [
    "Embedded feature selection methods involve integrating feature selection into the model training process itself. Common techniques include:\n",
    "* L1 Regularization (Lasso): Penalizes certain coefficients to encourage sparsity in feature selection.\n",
    "* Decision Tree-based methods: Decision trees can assess feature importance during training.\n",
    "* Recursive Feature Elimination (RFE): Iteratively removes the least important features during model training.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset of houses with features 'size', 'location', and 'age'. We can use LASSO regression to automatically select relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902ed857-1f75-48bd-8c00-0f7d31fb8c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features are:\n",
      "size\n",
      "age\n",
      "price\n",
      "location_suburban\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "data = pd.DataFrame({\n",
    "    'size': [1500, 1800, 1200, 2000, 1600],\n",
    "    'location': ['urban', 'suburban', 'urban', 'rural', 'suburban'],\n",
    "    'age': [10, 5, 20, 2, 15],\n",
    "    'price': [200000, 220000, 180000, 250000, 210000]\n",
    "})\n",
    "data = pd.get_dummies(data, columns=['location'], drop_first=True)\n",
    "x = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "lasso =Lasso(alpha = 0.01)\n",
    "lasso.fit(x,y)\n",
    "selected = lasso.coef_\n",
    "print('Selected Features are:')\n",
    "for feature, coef in zip(data.columns, selected):\n",
    "    if coef != 0:\n",
    "        print(f\"{feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b12cb-91a5-4e9d-a10c-f3f4fb540242",
   "metadata": {},
   "source": [
    "#### Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe0203-dcdc-492a-bfeb-be9c21041b10",
   "metadata": {},
   "source": [
    "* The Filter method doesn't consider feature interactions, which can limit its effectiveness.\n",
    "* It may remove redundant features but not necessarily the most relevant ones.\n",
    "* It assumes that all features are equally important for all learning algorithms, which might not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c86cd-68cc-4527-877b-f068c39aa9e1",
   "metadata": {},
   "source": [
    "#### Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844643e-a56e-4c86-aa85-ec1ae2c09f51",
   "metadata": {},
   "source": [
    "The Filter method is preferred when computational efficiency is important and when we want a quick initial assessment of feature importance without involving complex model training. It can also be useful for data exploration and preliminary insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6bebb-bff6-4758-b909-45b66c7fcf6e",
   "metadata": {},
   "source": [
    "#### Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e1438-0d30-45d8-895c-1cd34f5c72cb",
   "metadata": {},
   "source": [
    "In this case, we could use the Filter method by calculating correlations or mutual information between each feature and the target variable (churn). Features with higher correlation or mutual information scores are more likely to be relevant. We might also consider calculating variance to identify features with low variability. Based on these scores, we can rank the features and select the top ones as potential candidates for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd02c22-1f4b-4202-b318-5bc4fb28dd52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features are ['total_usage', 'churn']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, 30, 40, 22, 28],\n",
    "    'contract_length': [12, 24, 6, 18, 36],\n",
    "    'total_usage': [100, 150, 80, 200, 120],\n",
    "    'churn': [0, 1, 0, 1, 0]\n",
    "})\n",
    "cor = data.corr()['churn'].abs()\n",
    "selected = cor[cor>0.7].index.tolist()\n",
    "print('Selected Features are', selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c81e0-c618-4038-8223-52a646cc153b",
   "metadata": {},
   "source": [
    "#### Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9b260-e10f-47a9-9de3-5122f26ce649",
   "metadata": {},
   "source": [
    "In the Embedded method, we can incorporate feature selection during the model training process. For soccer match prediction, we could use a machine learning algorithm like Random Forest or Gradient Boosting, which inherently provide feature importance scores. By training these models on our dataset, we can extract the feature importance scores and identify which player statistics, team rankings, or other features contribute the most to predicting match outcomes. Features with higher importance scores are more relevant for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e213a6a9-ea3f-4972-8bd9-38d232a0772a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features are:\n",
      "player_stats\n",
      "team_ranking\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "data = pd.DataFrame({\n",
    "    'player_stats': [10, 8, 5, 7, 9],\n",
    "    'team_ranking': [3, 1, 5, 2, 4],\n",
    "    'outcome': ['win', 'win', 'loss', 'win', 'loss']\n",
    "})\n",
    "x = data.drop('outcome', axis=1)\n",
    "y = data['outcome']\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x,y)\n",
    "selected = rf.feature_importances_\n",
    "print('Selected Features are:')\n",
    "for feature, coef in zip(x.columns, selected):\n",
    "    if coef != 0:\n",
    "        print(f\"{feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e77471-fed1-40a1-97e3-1689d9878232",
   "metadata": {},
   "source": [
    "#### Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553a2b5-f242-49fb-9766-d75a2480658c",
   "metadata": {},
   "source": [
    "In the Wrapper method, we can employ techniques like Recursive Feature Elimination (RFE) or forward/backward selection. We would start with a subset of features and iteratively train the predictive model. After each iteration, we assess the model's performance using cross-validation or a separate validation set. We remove the least important feature and continue iterating until a stopping criterion is met. This way, the Wrapper method helps we find the best subset of features that maximizes the model's predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ad61ce-0fdb-4c82-af6d-4e59dd9ff7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Featues are: [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "data = pd.DataFrame({\n",
    "    'size': [1500, 1800, 1200, 2000, 1600],\n",
    "    'location': ['urban', 'suburban', 'urban', 'rural', 'suburban'],\n",
    "    'age': [10, 5, 20, 2, 15],\n",
    "    'price': [200000, 220000, 180000, 250000, 210000]\n",
    "})\n",
    "data = pd.get_dummies(data, columns=['location'], drop_first=True)\n",
    "x = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "rfe = RFE(lr(),n_features_to_select = 2)\n",
    "selected = rfe.fit_transform(x,y)\n",
    "print('Selected Featues are:',selected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
