{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47a8557-a473-4459-88e6-a5d412c08492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d2258-64fe-4567-9d3c-083c5dec0349",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006e140-b773-459d-be4c-f1e385fbd829",
   "metadata": {},
   "source": [
    "Min-Max Scaling is a data preprocessing technique used to scale numeric features in a dataset to a specific range, usually [0, 1]. It works by subtracting the minimum value from each data point and then dividing it by the range (difference between the maximum and minimum values). This technique ensures that all features are on the same scale, which can improve the performance of certain machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a561f07-5998-4182-92e0-4f6ae0b14583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.77777778]\n",
      " [1.         0.        ]\n",
      " [0.         0.44444444]\n",
      " [0.66666667 0.11111111]\n",
      " [0.2        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = pd.DataFrame({'price': [200, 300, 150, 250, 180],'rating': [4.5, 3.8, 4.2, 3.9, 4.7]})\n",
    "s = MinMaxScaler()\n",
    "scaled = s.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef03c27-4d44-4773-b182-9afd1411388d",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204f49d-b434-45d9-a0d6-ccc7cb978c99",
   "metadata": {},
   "source": [
    "Unit Vector technique scales each feature to have a unit norm (length). Unlike Min-Max Scaling, which brings the values within a specific range, Unit Vector technique scales each data point so that its Euclidean norm becomes 1. It's particularly useful when the magnitude of each feature is not important, and the direction matters more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a871f3c9-e5f3-46f6-b3c1-0a6ff3df26f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99974697 0.02249431]\n",
      " [0.99991979 0.01266565]\n",
      " [0.99960823 0.02798903]\n",
      " [0.99987834 0.0155981 ]\n",
      " [0.99965928 0.02610221]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "data = pd.DataFrame({'price': [200, 300, 150, 250, 180],'rating': [4.5, 3.8, 4.2, 3.9, 4.7]})\n",
    "n = Normalizer()\n",
    "scaled = n.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0543fc-f63a-4deb-978d-0fd9c7c1617b",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f65d2-0842-4fc8-a040-242f9a99eea2",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It achieves this by finding the principal components (linear combinations of original features) that explain the most variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf6132a-e622-45fd-b5a3-e2ed0e35502e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Dimensionalities are combined:\n",
      " [[-16.00114271]\n",
      " [ 84.00104663]\n",
      " [-65.99917148]\n",
      " [ 34.00113115]\n",
      " [-36.00186358]]\n",
      "When Dimensionalities are NOT combined:\n",
      " [[-1.60011427e+01 -2.04528839e-01]\n",
      " [ 8.40010466e+01  2.37880848e-02]\n",
      " [-6.59991715e+01  3.31305469e-01]\n",
      " [ 3.40011311e+01  1.59626842e-01]\n",
      " [-3.60018636e+01 -3.10191556e-01]]\n"
     ]
    }
   ],
   "source": [
    "## Example\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "data = pd.DataFrame({'price': [200, 300, 150, 250, 180],'rating': [4.5, 3.8, 4.2, 3.9, 4.7]})\n",
    "p = PCA(n_components=1)\n",
    "c = PCA()\n",
    "scaled = p.fit_transform(data)\n",
    "s = c.fit_transform(data)\n",
    "print(\"When Dimensionalities are combined:\\n\",scaled)\n",
    "print(\"When Dimensionalities are NOT combined:\\n\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d824547-fffc-428e-af40-adb538fef130",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9900c24-44de-4f47-9852-8b60a8cea09e",
   "metadata": {},
   "source": [
    "PCA can be used for Feature Extraction by identifying and selecting the most important principal components as new features. These components capture the most significant variability in the data. By selecting a smaller number of principal components, we can effectively reduce the dimensionality of the dataset while retaining important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d726360-c8d6-49a7-8da0-09a29238ebc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal components (directions) [[ 0.99998888 -0.00471675]]\n",
      "Variance explained by each component [0.99998061]\n",
      "Principal components (directions) [[ 0.99998888 -0.00471675]\n",
      " [-0.00471675 -0.99998888]]\n",
      "Variance explained by each component [9.99980606e-01 1.93944305e-05]\n"
     ]
    }
   ],
   "source": [
    "## Example\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "data = pd.DataFrame({'price': [200, 300, 150, 250, 180],'rating': [4.5, 3.8, 4.2, 3.9, 4.7]})\n",
    "p = PCA(n_components=1)\n",
    "c = PCA()\n",
    "scaled = p.fit_transform(data)\n",
    "s = c.fit_transform(data)\n",
    "print(\"Principal components (directions)\",p.components_)\n",
    "print(\"Variance explained by each component\",p.explained_variance_ratio_)\n",
    "print(\"Principal components (directions)\",c.components_)\n",
    "print(\"Variance explained by each component\",c.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089ddff-2ba6-428e-8353-afad7dcc56f5",
   "metadata": {},
   "source": [
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405a7eb-0604-4cfd-8070-6263b2f318a3",
   "metadata": {},
   "source": [
    "For the recommendation system project, Min-Max scaling can be used to preprocess features like price, rating, and delivery time. This would ensure that each feature is on the same scale, allowing the recommendation algorithm to give appropriate weight to each feature without one dominating over the others due to their different magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211f258-d52b-4fde-b314-e71bf4d48e54",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1b7c7-b6f7-4b21-85fe-e85602136de9",
   "metadata": {},
   "source": [
    "For the stock price prediction project, PCA can be used to reduce the dimensionality of the dataset by identifying the principal components that capture the most variability in the stock-related features. By retaining a smaller number of principal components, we can simplify the model and reduce the risk of overfitting while preserving the essence of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65b28e-bc19-41cb-88c0-47538e287738",
   "metadata": {},
   "source": [
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e96076-cc4e-449a-9c8c-9754b5845633",
   "metadata": {},
   "source": [
    "For the given dataset [1, 5, 10, 15, 20], to transform the values to a range of -1 to 1 using Min-Max scaling, we can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262b642b-5e5b-4f43-9d9e-599fe787d3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Calculation: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n",
      "Auto Calculation: [[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as ny\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = ny.array([1,5,10,15,20])\n",
    "mn = ny.min(data)\n",
    "mx = ny.max(data)\n",
    "mms = -1 + 2*(data-mn) / (mx-mn)\n",
    "scaled = MinMaxScaler(feature_range=(-1,1)).fit_transform(data.reshape(-1,1))\n",
    "print(\"Manual Calculation:\",mms)\n",
    "print(\"Auto Calculation:\",scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e0f37-1c82-4b9a-8aab-25e6e4e59da4",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6ce32-3f6a-4899-b5a2-dd96b9b34e48",
   "metadata": {},
   "source": [
    "The number of principal components to retain in PCA depends on the desired level of variance preservation. We can use the cumulative explained variance ratio to determine the number of components that collectively explain a sufficient portion of the total variance. A common approach is to choose the number of components that explain a high percentage of variance, like 95% or 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9cfa44-4ca8-459d-8d71-4a0704b16b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component [6.15052684e-01 3.84947316e-01 4.53459435e-31]\n",
      "Cumulative Variance [0.61505268 1.         1.        ]\n",
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import numpy as ny\n",
    "from sklearn.decomposition import PCA\n",
    "data = ny.array([[170, 65, 30, 0, 120],[160, 55, 25, 1, 130],[155, 55, 22, 0, 110]])\n",
    "p = PCA()\n",
    "p.fit_transform(data)\n",
    "cv = ny.cumsum(p.explained_variance_ratio_)\n",
    "print(\"Variance explained by each component\",p.explained_variance_ratio_)\n",
    "print(\"Cumulative Variance\",cv)\n",
    "dv = 0.95\n",
    "num_comp = ny.argmax(cv>= dv) +1\n",
    "print(\"Number of principal components to retain:\", num_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
