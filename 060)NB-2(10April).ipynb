{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fced175-1a01-408c-95b3-0153a28b2944",
   "metadata": {},
   "source": [
    "# Naïve bayes-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309d6e3-e31c-498b-b7c8-37026930bd42",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan? Show using python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f8bf2-de3c-494c-85de-904f90601a69",
   "metadata": {},
   "source": [
    "To calculate the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem:\n",
    "**P(Smoker∣Uses Health Insurance) = {P(Uses Health Insurance∣Smoker)⋅P(Smoker)} / P(Uses Health Insurance)**\n",
    "\n",
    "Given:\n",
    "* P(Uses Health Insurance)=0.70 (70%)\n",
    "* P(Smoker∣Uses Health Insurance)=?\n",
    "* P(Smoker)=? To find\n",
    "* P(Uses Health Insurance∣Smoker)=0.40 (40%)\n",
    "We need to find P(Smoker) to complete the calculation. Assuming that the survey represents the overall employee population, we can calculate P(Smoker) as the proportion of smokers in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208f47f7-5f3d-4b7d-9dac-3bf6b86b6def",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an employee is a smoker given that they use the health insurance plan is: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_uses_health_insurance = 0.70\n",
    "P_uses_health_insurance_given_smoker = 0.40\n",
    "\n",
    "# Calculate P(Smoker) using the law of total probability\n",
    "P_smoker = P_uses_health_insurance * P_uses_health_insurance_given_smoker / P_uses_health_insurance\n",
    "\n",
    "# Calculate P(Smoker | Uses Health Insurance) using Bayes' theorem\n",
    "P_smoker_given_uses_health_insurance = (P_uses_health_insurance_given_smoker * P_smoker) / P_uses_health_insurance\n",
    "print(f\"The probability that an employee is a smoker given that they use the health insurance plan is: {P_smoker_given_uses_health_insurance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfb381-953b-4828-9760-32ab958e0ea6",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508319b9-94c5-48a8-a610-380ab210a9a3",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of data they are suited for and how they model feature probabilities:\n",
    "* **Bernoulli Naive Bayes** is typically used for binary feature data, where each feature can take one of two values (e.g., 0 or 1, True or False). It models the presence or absence of features and calculates probabilities based on binary counts.\n",
    "* **Multinomial Naive Bayes**, on the other hand, is suitable for discrete data where features represent counts or frequencies of events. It is commonly used in text classification tasks, where the features are often word counts or term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa95ff2-11eb-46f5-b0ca-2d0d004904a6",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5207f-eab4-4aac-921d-9030e27ad1cd",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes typically assumes that missing values are informative, meaning that their absence conveys information about the data. In practice, this can lead to issues, especially if missing values are not informative but simply the result of data collection or recording errors.\n",
    "\n",
    "Handling missing values in Bernoulli Naive Bayes can be challenging. Here are a few common approaches:\n",
    "* **Imputation:** We can impute missing values with a specific value (e.g., 0 or 1) to indicate their presence or absence. However, this approach may introduce bias and may not be appropriate if missingness is not informative.\n",
    "* **Ignoring Missing Data:** We can choose to ignore instances with missing values during training and classification. This approach may work if the proportion of missing data is small and doesn't significantly impact the analysis.\n",
    "* **Model-Based Imputation:** Use more advanced techniques like logistic regression or other models to predict missing values based on available data. This can be useful when missingness is related to other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80647c6f-c1af-4b81-a186-9cd04b67aee3",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447d0b0-7498-4c90-b62d-70c11bffefed",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that continuous features follow a Gaussian (normal) distribution. It can handle multi-class classification problems by modeling the distribution of each feature for each class and then making predictions based on these distributions.\n",
    "\n",
    "In a multi-class setting, the algorithm calculates the probability of each class for a given set of feature values and assigns the instance to the class with the highest probability. This makes Gaussian Naive Bayes suitable for problems where the features are continuous and the classes are discrete and can be extended to handle more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e1216-4b5f-4466-a4bf-bf2c5107a43e",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "* **Data preparation:**\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "* **Implementation:**\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "* **Results:** Report the following performance metrics for each classifier:\n",
    "    * Accuracy\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1 score\n",
    "    \n",
    "* **Discussion:**\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "* **Conclusion:**\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "**Note:** *This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64feebb3-e418-4715-934c-33f64efccb41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4601 entries, (0.0, 0.64) to (0.0, 0.0)\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   word_0                      4601 non-null   float64\n",
      " 1   word_1                      4601 non-null   float64\n",
      " 2   word_2                      4601 non-null   float64\n",
      " 3   word_3                      4601 non-null   float64\n",
      " 4   word_4                      4601 non-null   float64\n",
      " 5   word_5                      4601 non-null   float64\n",
      " 6   word_6                      4601 non-null   float64\n",
      " 7   word_7                      4601 non-null   float64\n",
      " 8   word_8                      4601 non-null   float64\n",
      " 9   word_9                      4601 non-null   float64\n",
      " 10  word_10                     4601 non-null   float64\n",
      " 11  word_11                     4601 non-null   float64\n",
      " 12  word_12                     4601 non-null   float64\n",
      " 13  word_13                     4601 non-null   float64\n",
      " 14  word_14                     4601 non-null   float64\n",
      " 15  word_15                     4601 non-null   float64\n",
      " 16  word_16                     4601 non-null   float64\n",
      " 17  word_17                     4601 non-null   float64\n",
      " 18  word_18                     4601 non-null   float64\n",
      " 19  word_19                     4601 non-null   float64\n",
      " 20  word_20                     4601 non-null   float64\n",
      " 21  word_21                     4601 non-null   float64\n",
      " 22  word_22                     4601 non-null   float64\n",
      " 23  word_23                     4601 non-null   float64\n",
      " 24  word_24                     4601 non-null   float64\n",
      " 25  word_25                     4601 non-null   float64\n",
      " 26  word_26                     4601 non-null   float64\n",
      " 27  word_27                     4601 non-null   float64\n",
      " 28  word_28                     4601 non-null   float64\n",
      " 29  word_29                     4601 non-null   float64\n",
      " 30  word_30                     4601 non-null   float64\n",
      " 31  word_31                     4601 non-null   float64\n",
      " 32  word_32                     4601 non-null   float64\n",
      " 33  word_33                     4601 non-null   float64\n",
      " 34  word_34                     4601 non-null   float64\n",
      " 35  word_35                     4601 non-null   float64\n",
      " 36  word_36                     4601 non-null   float64\n",
      " 37  word_37                     4601 non-null   float64\n",
      " 38  word_38                     4601 non-null   float64\n",
      " 39  word_39                     4601 non-null   float64\n",
      " 40  word_40                     4601 non-null   float64\n",
      " 41  word_41                     4601 non-null   float64\n",
      " 42  word_42                     4601 non-null   float64\n",
      " 43  word_43                     4601 non-null   float64\n",
      " 44  word_44                     4601 non-null   float64\n",
      " 45  word_45                     4601 non-null   float64\n",
      " 46  word_46                     4601 non-null   float64\n",
      " 47  word_47                     4601 non-null   float64\n",
      " 48  char_exclamation            4601 non-null   float64\n",
      " 49  char_dollar                 4601 non-null   float64\n",
      " 50  char_parenthesis            4601 non-null   float64\n",
      " 51  char_bracket                4601 non-null   float64\n",
      " 52  capital_run_length_average  4601 non-null   float64\n",
      " 53  capital_run_length_longest  4601 non-null   int64  \n",
      " 54  capital_run_length_total    4601 non-null   int64  \n",
      " 55  is_spam                     4601 non-null   int64  \n",
      "dtypes: float64(53), int64(3)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as ny\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score as cv\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Loading Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "col = [f'word_{i}' for i in range(48)] + \\\n",
    "        ['char_exclamation', 'char_dollar', 'char_parenthesis', 'char_bracket', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', 'is_spam']\n",
    "ds = pd.read_csv(url, header=None, names=col)\n",
    "print(ds.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac6e27-d99c-4248-ad77-bef83ffe0215",
   "metadata": {},
   "source": [
    "No Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b231163f-cd4d-4611-8769-b262d78a13dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for Bernoulli Naive Bayes\n",
      " Accuracy: 0.89\n",
      " Precision: 0.70\n",
      " Recall: 0.96\n",
      " F1 Score: 0.81\n",
      "\n",
      "Performance metrics for Multinomial Naive Bayes\n",
      " Accuracy: 0.78\n",
      " Precision: 0.70\n",
      " Recall: 0.96\n",
      " F1 Score: 0.81\n",
      "\n",
      "Performance metrics for Gaussian Naive Bayes\n",
      " Accuracy: 0.82\n",
      " Precision: 0.70\n",
      " Recall: 0.96\n",
      " F1 Score: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spliting data\n",
    "x = ds.drop('is_spam',axis=1)\n",
    "y = ds['is_spam']\n",
    "\n",
    "# Implementing various types of Bayes classifiers\n",
    "models={\"Bernoulli Naive Bayes\":BernoulliNB(),\"Multinomial Naive Bayes\": MultinomialNB(),\"Gaussian Naive Bayes\": GaussianNB()}\n",
    "\n",
    "# Useing 10-fold cross-validation to evaluate the performance of each classifier\n",
    "results = {}\n",
    "for mn, m in models.items():\n",
    "    scores = cv(m,x,y,cv=10,scoring=\"accuracy\")\n",
    "    results[mn] = scores\n",
    "    \n",
    "# Report performance metrics\n",
    "for mn, scores in results.items():\n",
    "    print(f\"Performance metrics for {mn}\")\n",
    "    print(f\" Accuracy: {ny.mean(scores):.2f}\")\n",
    "    print(f\" Precision: {ny.mean(cv(m, x, y, cv=10, scoring='precision')):.2f}\")\n",
    "    print(f\" Recall: {ny.mean(cv(m, x, y, cv=10, scoring='recall')):.2f}\")\n",
    "    print(f\" F1 Score: {ny.mean(cv(m, x, y, cv=10, scoring='f1')):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752f30b-fe2d-4e03-89a1-86ea05410daa",
   "metadata": {},
   "source": [
    "* **Discussion:**\n",
    "    * **Performance Comparison:** All three variants of Naive Bayes (Bernoulli, Multinomial, and Gaussian) have similar performance in terms of accuracy, precision, recall, and F1 score. They achieve high recall (ability to identify spam messages) but moderate precision (proportion of predicted spam that is actually spam), which indicates that they tend to classify some non-spam messages as spam.\n",
    "    * **Precision vs. Recall Trade-off:** The models have relatively low precision, possibly due to the conservative nature of classifying messages as spam to avoid missing any actual spam (high recall). This trade-off between precision and recall is a common challenge in spam classification.\n",
    "    * **Limitations of Naive Bayes:** Naive Bayes assumes independence between features, which may not hold for text data like email messages. This assumption can limit its ability to capture complex dependencies between words in messages.\n",
    "* **Conclusion:**\n",
    "    * All three Naive Bayes variants (Bernoulli, Multinomial, Gaussian) achieved similar performance on the \"Spambase\" dataset, with high recall but lower precision.\n",
    "    * The choice of the best Naive Bayes variant depends on the specific goals and requirements of the spam classification task. If avoiding false negatives (missing spam) is critical, then high recall is preferred, as seen in these models. However, if minimizing false positives (classifying non-spam as spam) is more important, then precision could be improved with further tuning or by considering other algorithms.\n",
    "\n",
    "Overall, while Naive Bayes can be a good starting point for spam classification, it may require further refinement and exploration of alternative approaches to achieve better precision without sacrificing recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
